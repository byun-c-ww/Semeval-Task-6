{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1865518c",
      "metadata": {
        "id": "1865518c"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import seaborn as sns\n",
        "import string\n",
        "import re\n",
        "# sns.set()\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Graphics in retina format are more sharp and legible\n",
        "%config InlineBackend.figure_format = 'retina'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c5dc252a",
      "metadata": {
        "id": "c5dc252a",
        "outputId": "49dc85c3-05fc-47ec-ad09-20d4b7db41c6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hyp</th>\n",
              "      <th>ref</th>\n",
              "      <th>src</th>\n",
              "      <th>tgt</th>\n",
              "      <th>model</th>\n",
              "      <th>task</th>\n",
              "      <th>labels</th>\n",
              "      <th>label</th>\n",
              "      <th>p(Hallucination)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A sloping top .</td>\n",
              "      <td>tgt</td>\n",
              "      <td>The sides of the casket were covered with heav...</td>\n",
              "      <td>A decorative feature that sits on top of somet...</td>\n",
              "      <td>ltg/flan-t5-definition-en-base</td>\n",
              "      <td>DM</td>\n",
              "      <td>[Not Hallucination, Hallucination, Not Halluci...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>To react too much .</td>\n",
              "      <td>tgt</td>\n",
              "      <td>Please try not to overreact if she drives badl...</td>\n",
              "      <td>To react too much or too intensely .</td>\n",
              "      <td>ltg/flan-t5-definition-en-base</td>\n",
              "      <td>DM</td>\n",
              "      <td>[Not Hallucination, Not Hallucination, Not Hal...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The process of spoiling ; the state of being s...</td>\n",
              "      <td>tgt</td>\n",
              "      <td>To prevent spoilage , store in a cool , dry pl...</td>\n",
              "      <td>The process of spoiling .</td>\n",
              "      <td>ltg/flan-t5-definition-en-base</td>\n",
              "      <td>DM</td>\n",
              "      <td>[Hallucination, Not Hallucination, Hallucinati...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>To arrange in a particular way .</td>\n",
              "      <td>tgt</td>\n",
              "      <td>The way the opposition has framed the argument...</td>\n",
              "      <td>To construct in words so as to establish a con...</td>\n",
              "      <td>ltg/flan-t5-definition-en-base</td>\n",
              "      <td>DM</td>\n",
              "      <td>[Hallucination, Not Hallucination, Not Halluci...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A feeling of concern ; a feeling of anxiety .</td>\n",
              "      <td>tgt</td>\n",
              "      <td>To mix with thy concernments i desist . What i...</td>\n",
              "      <td>That in which one is concerned or interested ;...</td>\n",
              "      <td>ltg/flan-t5-definition-en-base</td>\n",
              "      <td>DM</td>\n",
              "      <td>[Not Hallucination, Hallucination, Hallucinati...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 hyp  ref  \\\n",
              "0                                    A sloping top .  tgt   \n",
              "1                                To react too much .  tgt   \n",
              "2  The process of spoiling ; the state of being s...  tgt   \n",
              "3                   To arrange in a particular way .  tgt   \n",
              "4      A feeling of concern ; a feeling of anxiety .  tgt   \n",
              "\n",
              "                                                 src  \\\n",
              "0  The sides of the casket were covered with heav...   \n",
              "1  Please try not to overreact if she drives badl...   \n",
              "2  To prevent spoilage , store in a cool , dry pl...   \n",
              "3  The way the opposition has framed the argument...   \n",
              "4  To mix with thy concernments i desist . What i...   \n",
              "\n",
              "                                                 tgt  \\\n",
              "0  A decorative feature that sits on top of somet...   \n",
              "1               To react too much or too intensely .   \n",
              "2                          The process of spoiling .   \n",
              "3  To construct in words so as to establish a con...   \n",
              "4  That in which one is concerned or interested ;...   \n",
              "\n",
              "                            model task  \\\n",
              "0  ltg/flan-t5-definition-en-base   DM   \n",
              "1  ltg/flan-t5-definition-en-base   DM   \n",
              "2  ltg/flan-t5-definition-en-base   DM   \n",
              "3  ltg/flan-t5-definition-en-base   DM   \n",
              "4  ltg/flan-t5-definition-en-base   DM   \n",
              "\n",
              "                                              labels  label  p(Hallucination)  \n",
              "0  [Not Hallucination, Hallucination, Not Halluci...      1               0.6  \n",
              "1  [Not Hallucination, Not Hallucination, Not Hal...      0               0.0  \n",
              "2  [Hallucination, Not Hallucination, Hallucinati...      1               0.6  \n",
              "3  [Hallucination, Not Hallucination, Not Halluci...      1               0.6  \n",
              "4  [Not Hallucination, Hallucination, Hallucinati...      1               0.6  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# use either top model-aware or model-agnostic\n",
        "# df = pd.read_json('../SHROOM_dev-v2_validation/val.model-agnostic.json')\n",
        "df = pd.read_json('data/val.model-aware.v2.json')\n",
        "df[\"label\"] = df[\"label\"].map({\"Hallucination\":1, \"Not Hallucination\":0})\n",
        "\n",
        "df = df.dropna(how=\"any\", axis=1)\n",
        "df.head()\n",
        "\n",
        "print(df['label'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "40f35ffa",
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('omw-1.4')\n",
        "# nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "80dbbac9",
      "metadata": {
        "id": "80dbbac9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "example text url email example example com\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# stemmer = nltk.SnowballStemmer(\"english\")\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    '''\n",
        "    Make text lowercase, remove text in square brackets, remove links, remove punctuation,\n",
        "    and remove words containing numbers.\n",
        "    '''\n",
        "    text = str(text).lower()\n",
        "    \n",
        "    # Remove text in square brackets\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    \n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    \n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    \n",
        "    # Remove punctuation\n",
        "    text = re.sub(rf'[{re.escape(string.punctuation)}]', ' ', text)\n",
        "    \n",
        "    # Remove new lines\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    \n",
        "    # Remove words containing numbers\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def preprocess_data(text):\n",
        "    text = clean_text(text)  # Clean punctuation, URLs, and so on\n",
        "    \n",
        "    # Remove stopwords\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "    \n",
        "    # Lemmatize all the words in the sentence\n",
        "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "example_text = \"This is an example text with a URL: http://example.com and an email: example@example.com.\"\n",
        "cleaned_text = preprocess_data(example_text)\n",
        "print(cleaned_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c0c74568",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'decorative feature sits top something'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_preprocessed = preprocess_data(\"A decorative feature that sits on top of something .\")\n",
        "example_preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4125ed4d",
      "metadata": {
        "id": "4125ed4d",
        "outputId": "c4697332-cc8c-45b5-a147-8dd1f3847f50"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hyp</th>\n",
              "      <th>ref</th>\n",
              "      <th>src</th>\n",
              "      <th>tgt</th>\n",
              "      <th>model</th>\n",
              "      <th>task</th>\n",
              "      <th>labels</th>\n",
              "      <th>label</th>\n",
              "      <th>p(Hallucination)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sloping top</td>\n",
              "      <td>tgt</td>\n",
              "      <td>side casket covered heavy black broadcloth vel...</td>\n",
              "      <td>decorative feature sits top something</td>\n",
              "      <td>ltg/flan-t5-definition-en-base</td>\n",
              "      <td>DM</td>\n",
              "      <td>[Not Hallucination, Hallucination, Not Halluci...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>react much</td>\n",
              "      <td>tgt</td>\n",
              "      <td>please try overreact drive badly first learnin...</td>\n",
              "      <td>react much intensely</td>\n",
              "      <td>ltg/flan-t5-definition-en-base</td>\n",
              "      <td>DM</td>\n",
              "      <td>[Not Hallucination, Not Hallucination, Not Hal...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>process spoiling state spoilt</td>\n",
              "      <td>tgt</td>\n",
              "      <td>prevent spoilage store cool dry place meaning ...</td>\n",
              "      <td>process spoiling</td>\n",
              "      <td>ltg/flan-t5-definition-en-base</td>\n",
              "      <td>DM</td>\n",
              "      <td>[Hallucination, Not Hallucination, Hallucinati...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>arrange particular way</td>\n",
              "      <td>tgt</td>\n",
              "      <td>way opposition framed argument make hard u win...</td>\n",
              "      <td>construct word establish context understanding...</td>\n",
              "      <td>ltg/flan-t5-definition-en-base</td>\n",
              "      <td>DM</td>\n",
              "      <td>[Hallucination, Not Hallucination, Not Halluci...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>feeling concern feeling anxiety</td>\n",
              "      <td>tgt</td>\n",
              "      <td>mix thy concernments desist meaning concernment</td>\n",
              "      <td>one concerned interested concern affair interest</td>\n",
              "      <td>ltg/flan-t5-definition-en-base</td>\n",
              "      <td>DM</td>\n",
              "      <td>[Not Hallucination, Hallucination, Hallucinati...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>using gas fired device way stop people using n...</td>\n",
              "      <td>either</td>\n",
              "      <td>doonii fayyadamuun meeshaa geejibuun namootaba...</td>\n",
              "      <td>using ship transport good far efficient way mo...</td>\n",
              "      <td>facebook/nllb-200-distilled-600M</td>\n",
              "      <td>MT</td>\n",
              "      <td>[Hallucination, Hallucination, Hallucination, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>since montevideo located south equator summer ...</td>\n",
              "      <td>either</td>\n",
              "      <td>وبما أن مونتيفيديو موجودة في جنوب خط الاستواء،...</td>\n",
              "      <td>since montevideo south equator summer winter n...</td>\n",
              "      <td>facebook/nllb-200-distilled-600M</td>\n",
              "      <td>MT</td>\n",
              "      <td>[Not Hallucination, Not Hallucination, Not Hal...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>also supporter singapore deputy prime minister...</td>\n",
              "      <td>either</td>\n",
              "      <td>gin abiabi hiya han deputy prime minister han ...</td>\n",
              "      <td>greeted singapore deputy prime minister wong k...</td>\n",
              "      <td>facebook/nllb-200-distilled-600M</td>\n",
              "      <td>MT</td>\n",
              "      <td>[Hallucination, Hallucination, Not Hallucinati...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>concept worm parasitic organism trained behave...</td>\n",
              "      <td>either</td>\n",
              "      <td>འབུ་ཞེས་པའི་ཐ་སྙད་དེ་ཉིད་འབུ་སྲིན་དཔྱད་རིག་པ་བ...</td>\n",
              "      <td>term bug used entomologist formal sense group ...</td>\n",
              "      <td>facebook/nllb-200-distilled-600M</td>\n",
              "      <td>MT</td>\n",
              "      <td>[Hallucination, Hallucination, Hallucination, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>500</th>\n",
              "      <td>exception fact u government able negotiate con...</td>\n",
              "      <td>either</td>\n",
              "      <td>ihandjika iyakwece hali kulihana iya kutunguis...</td>\n",
              "      <td>liberal criticism reconstruction effort focuse...</td>\n",
              "      <td>facebook/nllb-200-distilled-600M</td>\n",
              "      <td>MT</td>\n",
              "      <td>[Not Hallucination, Not Hallucination, Halluci...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>501 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   hyp     ref  \\\n",
              "0                                          sloping top     tgt   \n",
              "1                                           react much     tgt   \n",
              "2                        process spoiling state spoilt     tgt   \n",
              "3                               arrange particular way     tgt   \n",
              "4                      feeling concern feeling anxiety     tgt   \n",
              "..                                                 ...     ...   \n",
              "496  using gas fired device way stop people using n...  either   \n",
              "497  since montevideo located south equator summer ...  either   \n",
              "498  also supporter singapore deputy prime minister...  either   \n",
              "499  concept worm parasitic organism trained behave...  either   \n",
              "500  exception fact u government able negotiate con...  either   \n",
              "\n",
              "                                                   src  \\\n",
              "0    side casket covered heavy black broadcloth vel...   \n",
              "1    please try overreact drive badly first learnin...   \n",
              "2    prevent spoilage store cool dry place meaning ...   \n",
              "3    way opposition framed argument make hard u win...   \n",
              "4      mix thy concernments desist meaning concernment   \n",
              "..                                                 ...   \n",
              "496  doonii fayyadamuun meeshaa geejibuun namootaba...   \n",
              "497  وبما أن مونتيفيديو موجودة في جنوب خط الاستواء،...   \n",
              "498  gin abiabi hiya han deputy prime minister han ...   \n",
              "499  འབུ་ཞེས་པའི་ཐ་སྙད་དེ་ཉིད་འབུ་སྲིན་དཔྱད་རིག་པ་བ...   \n",
              "500  ihandjika iyakwece hali kulihana iya kutunguis...   \n",
              "\n",
              "                                                   tgt  \\\n",
              "0                decorative feature sits top something   \n",
              "1                                 react much intensely   \n",
              "2                                     process spoiling   \n",
              "3    construct word establish context understanding...   \n",
              "4     one concerned interested concern affair interest   \n",
              "..                                                 ...   \n",
              "496  using ship transport good far efficient way mo...   \n",
              "497  since montevideo south equator summer winter n...   \n",
              "498  greeted singapore deputy prime minister wong k...   \n",
              "499  term bug used entomologist formal sense group ...   \n",
              "500  liberal criticism reconstruction effort focuse...   \n",
              "\n",
              "                                model task  \\\n",
              "0      ltg/flan-t5-definition-en-base   DM   \n",
              "1      ltg/flan-t5-definition-en-base   DM   \n",
              "2      ltg/flan-t5-definition-en-base   DM   \n",
              "3      ltg/flan-t5-definition-en-base   DM   \n",
              "4      ltg/flan-t5-definition-en-base   DM   \n",
              "..                                ...  ...   \n",
              "496  facebook/nllb-200-distilled-600M   MT   \n",
              "497  facebook/nllb-200-distilled-600M   MT   \n",
              "498  facebook/nllb-200-distilled-600M   MT   \n",
              "499  facebook/nllb-200-distilled-600M   MT   \n",
              "500  facebook/nllb-200-distilled-600M   MT   \n",
              "\n",
              "                                                labels  label  \\\n",
              "0    [Not Hallucination, Hallucination, Not Halluci...      1   \n",
              "1    [Not Hallucination, Not Hallucination, Not Hal...      0   \n",
              "2    [Hallucination, Not Hallucination, Hallucinati...      1   \n",
              "3    [Hallucination, Not Hallucination, Not Halluci...      1   \n",
              "4    [Not Hallucination, Hallucination, Hallucinati...      1   \n",
              "..                                                 ...    ...   \n",
              "496  [Hallucination, Hallucination, Hallucination, ...      1   \n",
              "497  [Not Hallucination, Not Hallucination, Not Hal...      0   \n",
              "498  [Hallucination, Hallucination, Not Hallucinati...      1   \n",
              "499  [Hallucination, Hallucination, Hallucination, ...      1   \n",
              "500  [Not Hallucination, Not Hallucination, Halluci...      0   \n",
              "\n",
              "     p(Hallucination)  \n",
              "0                 0.6  \n",
              "1                 0.0  \n",
              "2                 0.6  \n",
              "3                 0.6  \n",
              "4                 0.6  \n",
              "..                ...  \n",
              "496               1.0  \n",
              "497               0.0  \n",
              "498               0.8  \n",
              "499               1.0  \n",
              "500               0.4  \n",
              "\n",
              "[501 rows x 9 columns]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "columns = ['hyp','src','tgt'] # exclude task\n",
        "for x in columns:\n",
        "    df[x] = df[x].apply(preprocess_data)\n",
        "\n",
        "# X = np.reshape(X,(499))\n",
        "# X.shape, y.shape\n",
        "# X = df.drop(['label','labels', 'model', 'p(Hallucination)','task','ref'], axis=1)\n",
        "# y = df['label']\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "b76393a4",
      "metadata": {
        "id": "b76393a4"
      },
      "outputs": [],
      "source": [
        "# extract preprocessed data as CSV file,\n",
        "# also i extract them based on tasks, so that they can be fed separately\n",
        "df = df.drop(['labels'], axis=1)\n",
        "dm_df = df[df['task'] == 'DM']\n",
        "mt_df = df[df['task'] == 'MT']\n",
        "pg_df = df[df['task'] == 'PG']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "f543eb1e",
      "metadata": {
        "id": "f543eb1e",
        "outputId": "6433ff4b-5c0f-493a-f208-266b5d3469ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(188, 188, 125)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dm_df),len(mt_df), len(pg_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "7dedccc3",
      "metadata": {
        "id": "7dedccc3",
        "outputId": "40b4458c-2d53-44fa-aead-09287d131950"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "hyp                 0\n",
              "ref                 0\n",
              "src                 0\n",
              "tgt                 0\n",
              "model               0\n",
              "task                0\n",
              "label               0\n",
              "p(Hallucination)    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "number_of_na = pg_df.isna().sum()\n",
        "number_of_na"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "2d656088",
      "metadata": {
        "id": "2d656088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "# file_path = '/Users/byun/Desktop/School/semeval/eda_nlp/thesis_data_before/'\n",
        "file_path = r\"C:\\Users\\marko\\OneDrive\\바탕 화면\\semeval\\Task 6 - windows\\eda_nlp-20240601T173748Z-001\"\n",
        "file_path_uni = pathlib.Path(file_path).as_posix()\n",
        "print(file_path_uni)\n",
        "# Save the DataFrame as a CSV file at the specified location\n",
        "dm_df.to_csv(file_path_uni + '/dm_df.csv', index=False)\n",
        "mt_df.to_csv(file_path_uni + '/mt_df.csv', index=False)\n",
        "pg_df.to_csv(file_path_uni + '/pg_df.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "841ab711",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eda_path = r\"C:\\Users\\marko\\OneDrive\\바탕 화면\\semeval\\Task 6 - windows\\eda_nlp-20240601T173748Z-001\\eda_nlp\"\n",
        "eda_path_posix = pathlib.Path(eda_path).as_posix()\n",
        "eda_path_posix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b521f9dc",
      "metadata": {
        "id": "b521f9dc"
      },
      "outputs": [],
      "source": [
        "# C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/thesis_data_after\n",
        "# task PG, num_aug 16\n",
        "pg_rd_path = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/rd/pg_df_rd.csv'\n",
        "pg_ri_path = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/ri/pg_df_ri.csv'\n",
        "pg_rs_path = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/rs/pg_df_rs.csv'\n",
        "pg_sr_path = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/sr/pg_df_sr.csv'\n",
        "\n",
        "pg_rd = pd.read_csv(pg_rd_path)\n",
        "pg_ri = pd.read_csv(pg_ri_path)\n",
        "pg_rs = pd.read_csv(pg_rs_path)\n",
        "pg_sr = pd.read_csv(pg_sr_path)\n",
        "\n",
        "# num_aug 160\n",
        "path1 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/rd/pg_df_rd.csv'\n",
        "path2 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/ri/pg_df_ri.csv'\n",
        "path3 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/rs/pg_df_rs.csv'\n",
        "path4 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/sr/pg_df_sr.csv'\n",
        "\n",
        "pg_rd_160 = pd.read_csv(path1)\n",
        "pg_ri_160 = pd.read_csv(path2)\n",
        "pg_rs_160 = pd.read_csv(path3)\n",
        "pg_sr_160 = pd.read_csv(path4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8033caa1",
      "metadata": {
        "id": "8033caa1"
      },
      "outputs": [],
      "source": [
        "# task DM 16\n",
        "path1 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/rd/dm_df_rd.csv'\n",
        "path2 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/ri/dm_df_ri.csv'\n",
        "path3 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/rs/dm_df_rs.csv'\n",
        "path4 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/sr/dm_df_sr.csv'\n",
        "\n",
        "dm_rd = pd.read_csv(path1)\n",
        "dm_ri = pd.read_csv(path2)\n",
        "dm_rs = pd.read_csv(path3)\n",
        "dm_sr = pd.read_csv(path4)\n",
        "\n",
        "# 160\n",
        "path1 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after_160/rd/dm_df_rd.csv'\n",
        "path2 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after_160/ri/dm_df_ri.csv'\n",
        "path3 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after_160/rs/dm_df_rs.csv'\n",
        "path4 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after_160/sr/dm_df_sr.csv'\n",
        "\n",
        "dm_rd_160 = pd.read_csv(path1)\n",
        "dm_ri_160 = pd.read_csv(path2)\n",
        "dm_rs_160 = pd.read_csv(path3)\n",
        "dm_sr_160 = pd.read_csv(path4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9eea0f24",
      "metadata": {
        "id": "9eea0f24"
      },
      "outputs": [],
      "source": [
        "# task MT 16\n",
        "path1 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/rd/mt_df_rd.csv'\n",
        "path2 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/ri/mt_df_ri.csv'\n",
        "path3 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/rs/mt_df_rs.csv'\n",
        "path4 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/sr/mt_df_sr.csv'\n",
        "\n",
        "mt_rd = pd.read_csv(path1)\n",
        "mt_ri = pd.read_csv(path2)\n",
        "mt_rs = pd.read_csv(path3)\n",
        "mt_sr = pd.read_csv(path4)\n",
        "\n",
        "# 160\n",
        "path1 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after_160/rd/mt_df_rd.csv'\n",
        "path2 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after_160/ri/mt_df_ri.csv'\n",
        "path3 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after_160/rs/mt_df_rs.csv'\n",
        "path4 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after_160/sr/mt_df_sr.csv'\n",
        "\n",
        "mt_rd_160 = pd.read_csv(path1)\n",
        "mt_ri_160 = pd.read_csv(path2)\n",
        "mt_rs_160 = pd.read_csv(path3)\n",
        "mt_sr_160 = pd.read_csv(path4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "24240c67",
      "metadata": {
        "id": "24240c67"
      },
      "outputs": [],
      "source": [
        "#\n",
        "path1 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/rd/mt_df_rd.csv'\n",
        "path2 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/ri/mt_df_ri.csv'\n",
        "path3 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/rs/mt_df_rs.csv'\n",
        "path4 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/sr/mt_df_sr.csv'\n",
        "\n",
        "mt_rd = pd.read_csv(path1)\n",
        "mt_ri = pd.read_csv(path2)\n",
        "mt_rs = pd.read_csv(path3)\n",
        "mt_sr = pd.read_csv(path4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "76d0a9d4",
      "metadata": {
        "id": "76d0a9d4",
        "outputId": "83d49d31-7a74-4252-9062-1dcf715159de"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hyp</th>\n",
              "      <th>ref</th>\n",
              "      <th>src</th>\n",
              "      <th>tgt</th>\n",
              "      <th>model</th>\n",
              "      <th>label</th>\n",
              "      <th>p(Hallucination)</th>\n",
              "      <th>Unnamed: 7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>john john one</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>john ann like</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>john john one</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>john ann like</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>john john one</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>john ann like</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>john one</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>john ann</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>john john one</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>john ann like</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7849</th>\n",
              "      <td>dont money buy dictionary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>money buy dictionary</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7850</th>\n",
              "      <td>dont money buy dictionary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>buy dictionary</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7851</th>\n",
              "      <td>dont money buy dictionary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>money buy dictionary</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7852</th>\n",
              "      <td>dont money buy dictionary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>money buy dictionary</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7853</th>\n",
              "      <td>dont money buy dictionary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>money buy dictionary</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7854 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                            hyp     ref src                   tgt model  \\\n",
              "0                 john john one  either             john ann like         \n",
              "1                 john john one  either             john ann like         \n",
              "2                 john john one  either             john ann like         \n",
              "3                      john one  either                  john ann         \n",
              "4                 john john one  either             john ann like         \n",
              "...                         ...     ...  ..                   ...   ...   \n",
              "7849  dont money buy dictionary  either      money buy dictionary         \n",
              "7850  dont money buy dictionary  either            buy dictionary         \n",
              "7851  dont money buy dictionary  either      money buy dictionary         \n",
              "7852  dont money buy dictionary  either      money buy dictionary         \n",
              "7853  dont money buy dictionary  either      money buy dictionary         \n",
              "\n",
              "      label  p(Hallucination) Unnamed: 7  \n",
              "0         1               1.0             \n",
              "1         1               1.0             \n",
              "2         1               1.0             \n",
              "3         1               1.0             \n",
              "4         1               1.0             \n",
              "...     ...               ...        ...  \n",
              "7849      0               0.0             \n",
              "7850      0               0.0             \n",
              "7851      0               0.0             \n",
              "7852      0               0.0             \n",
              "7853      0               0.0             \n",
              "\n",
              "[7854 rows x 8 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# pg_rd = pg_rd.dropna(how=\"any\", axis=1)\n",
        "pg_rd = pg_rd.fillna('')\n",
        "pg_ri = pg_ri.fillna('')\n",
        "pg_rs = pg_rs.fillna('')\n",
        "pg_sr = pg_sr.fillna('')\n",
        "\n",
        "dm_rd = dm_rd.fillna('')\n",
        "dm_ri = dm_ri.fillna('')\n",
        "dm_rs = dm_rs.fillna('')\n",
        "dm_sr = dm_sr.fillna('')\n",
        "\n",
        "mt_rd = mt_rd.fillna('')\n",
        "mt_ri = mt_ri.fillna('')\n",
        "mt_rs = mt_rs.fillna('')\n",
        "mt_sr = mt_sr.fillna('')\n",
        "\n",
        "pg_rd_160 = pg_rd_160.fillna('')\n",
        "pg_ri_160 = pg_ri_160.fillna('')\n",
        "pg_rs_160 = pg_rs_160.fillna('')\n",
        "pg_sr_160 = pg_sr_160.fillna('')\n",
        "\n",
        "dm_rd_160 = dm_rd_160.fillna('')\n",
        "dm_ri_160 = dm_ri_160.fillna('')\n",
        "dm_rs_160 = dm_rs_160.fillna('')\n",
        "dm_sr_160 = dm_sr_160.fillna('')\n",
        "\n",
        "mt_rd_160 = mt_rd_160.fillna('')\n",
        "mt_ri_160 = mt_ri_160.fillna('')\n",
        "mt_rs_160 = mt_rs_160.fillna('')\n",
        "mt_sr_160 = mt_sr_160.fillna('')\n",
        "# drop na just deleted the model column i think\n",
        "mt_rd_160"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8c9dc490",
      "metadata": {
        "id": "8c9dc490"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hyp</th>\n",
              "      <th>ref</th>\n",
              "      <th>src</th>\n",
              "      <th>tgt</th>\n",
              "      <th>model</th>\n",
              "      <th>label</th>\n",
              "      <th>p(Hallucination)</th>\n",
              "      <th>Unnamed: 7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>saint john the apostle saint john the apostle one</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>trick ann like</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>john john</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>st john the apostle ann like</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>john john unitary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>st john ann like</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>john john unitary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>whoremonger ann like</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>john john peerless</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>john the divine ann like</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7849</th>\n",
              "      <td>dont money bargain dictionary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>money buy lexicon</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7850</th>\n",
              "      <td>dont money grease ones palms dictionary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>money purchase dictionary</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7851</th>\n",
              "      <td>dont money purchase dictionary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>money buy lexicon</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7852</th>\n",
              "      <td>dont money buy lexicon</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>money bargain dictionary</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7853</th>\n",
              "      <td>dont money buy dictionary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>money buy dictionary</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7854 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    hyp     ref src  \\\n",
              "0     saint john the apostle saint john the apostle one  either       \n",
              "1                                            john john   either       \n",
              "2                                     john john unitary  either       \n",
              "3                                     john john unitary  either       \n",
              "4                                    john john peerless  either       \n",
              "...                                                 ...     ...  ..   \n",
              "7849                      dont money bargain dictionary  either       \n",
              "7850            dont money grease ones palms dictionary  either       \n",
              "7851                     dont money purchase dictionary  either       \n",
              "7852                             dont money buy lexicon  either       \n",
              "7853                          dont money buy dictionary  either       \n",
              "\n",
              "                               tgt model  label  p(Hallucination) Unnamed: 7  \n",
              "0                   trick ann like            1               1.0             \n",
              "1     st john the apostle ann like            1               1.0             \n",
              "2                 st john ann like            1               1.0             \n",
              "3             whoremonger ann like            1               1.0             \n",
              "4         john the divine ann like            1               1.0             \n",
              "...                            ...   ...    ...               ...        ...  \n",
              "7849             money buy lexicon            0               0.0             \n",
              "7850     money purchase dictionary            0               0.0             \n",
              "7851             money buy lexicon            0               0.0             \n",
              "7852      money bargain dictionary            0               0.0             \n",
              "7853          money buy dictionary            0               0.0             \n",
              "\n",
              "[7854 rows x 8 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mt_sr_160"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "89942019",
      "metadata": {
        "id": "89942019"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hyp</th>\n",
              "      <th>ref</th>\n",
              "      <th>src</th>\n",
              "      <th>tgt</th>\n",
              "      <th>model</th>\n",
              "      <th>label</th>\n",
              "      <th>p(Hallucination)</th>\n",
              "      <th>Unnamed: 7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>john lackland john lackland one</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>john ann like</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>john john one and only</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>john ann like</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>whoremaster whoremaster one</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>john ann like</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>john john matchless</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>john ann like</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gospel according to john gospel according to j...</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>john ann like</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1117</th>\n",
              "      <td>dont money buy lexicon</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>money buy dictionary</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1118</th>\n",
              "      <td>dont money corrupt dictionary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>money buy dictionary</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1119</th>\n",
              "      <td>dont money grease ones palms dictionary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>money buy dictionary</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1120</th>\n",
              "      <td>dont money buy lexicon</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>money buy dictionary</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1121</th>\n",
              "      <td>dont money buy dictionary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>money buy dictionary</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1122 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    hyp     ref src  \\\n",
              "0                       john lackland john lackland one  either       \n",
              "1                                john john one and only  either       \n",
              "2                           whoremaster whoremaster one  either       \n",
              "3                                   john john matchless  either       \n",
              "4     gospel according to john gospel according to j...  either       \n",
              "...                                                 ...     ...  ..   \n",
              "1117                             dont money buy lexicon  either       \n",
              "1118                      dont money corrupt dictionary  either       \n",
              "1119            dont money grease ones palms dictionary  either       \n",
              "1120                             dont money buy lexicon  either       \n",
              "1121                          dont money buy dictionary  either       \n",
              "\n",
              "                       tgt model  label  p(Hallucination) Unnamed: 7  \n",
              "0            john ann like            1               1.0             \n",
              "1            john ann like            1               1.0             \n",
              "2            john ann like            1               1.0             \n",
              "3            john ann like            1               1.0             \n",
              "4            john ann like            1               1.0             \n",
              "...                    ...   ...    ...               ...        ...  \n",
              "1117  money buy dictionary            0               0.0             \n",
              "1118  money buy dictionary            0               0.0             \n",
              "1119  money buy dictionary            0               0.0             \n",
              "1120  money buy dictionary            0               0.0             \n",
              "1121  money buy dictionary            0               0.0             \n",
              "\n",
              "[1122 rows x 8 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mt_sr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3aa8a4ce",
      "metadata": {
        "id": "3aa8a4ce"
      },
      "outputs": [],
      "source": [
        "# original dataset before data aug\n",
        "# X, y = pg_df['hyp'], pg_df['label']\n",
        "# X, y = dm_df['hyp'], dm_df['label']\n",
        "# X, y = mt_df['hyp'], mt_df['label']\n",
        "\n",
        "# task PG\n",
        "# X, y = pg_rd[['hyp','tgt']] , pg_rd['label']\n",
        "# X, y = pg_ri[['hyp','tgt']] , pg_ri['label']\n",
        "# X, y = pg_rs[['hyp','tgt']] , pg_rs['label']\n",
        "# X, y = pg_sr[['hyp','tgt']] , pg_sr['label']\n",
        "# task DM\n",
        "# X, y = dm_rd[['hyp','tgt']] , dm_rd['label']\n",
        "# X, y = dm_ri[['hyp','tgt']], dm_ri['label']\n",
        "# X, y = dm_rs[['hyp','tgt']] , dm_rs['label']\n",
        "# X, y = dm_sr[['hyp','tgt']] , dm_sr['label']\n",
        "# task MT\n",
        "# X, y = mt_rd[['hyp','tgt']] , mt_rd['label']\n",
        "# X, y = mt_ri[['hyp','tgt']] , mt_ri['label']\n",
        "# X, y = mt_rs[['hyp','tgt']] , mt_rs['label']\n",
        "X, y = mt_sr[['hyp','tgt']] , mt_sr['label']\n",
        "\n",
        "# X, y = df['hyp'], df['label']\n",
        "# one way of getting label where task is DM, df.loc[df['task'] == 'DM', 'label']\n",
        "# X, y = df[df['task'] == 'PG']['hyp'], df[df['task'] == 'PG']['label']\n",
        "\n",
        "# task PG\n",
        "# X, y = pg_rd_160['hyp'] , pg_rd_160['label']\n",
        "# X, y = pg_ri_160['hyp'] , pg_ri_160['label']\n",
        "# X, y = pg_rs_160['hyp'] , pg_rs_160['label']\n",
        "# X, y = pg_sr_160['hyp'] , pg_sr_160['label']\n",
        "# task DM\n",
        "# X, y = dm_rd_160['hyp'] , dm_rd_160['label']\n",
        "# X, y = dm_ri_160['hyp'] , dm_ri_160['label']\n",
        "# X, y = dm_rs_160['hyp'] , dm_rs_160['label']\n",
        "# X, y = dm_sr_160['hyp'] , dm_sr_160['label']\n",
        "# task MT\n",
        "# X, y = mt_rd_160['hyp'] , mt_rd_160['label']\n",
        "# X, y = mt_ri_160['hyp'] , mt_ri_160['label']\n",
        "# X, y = mt_rs_160['hyp'] , mt_rs_160['label']\n",
        "# X, y = mt_sr_160['hyp'] , mt_sr_160['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "fa5e2fe1",
      "metadata": {
        "id": "fa5e2fe1",
        "outputId": "77ee464f-18ec-496c-e1a3-638ff5e05d93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class distribution:\n",
            " label\n",
            "0    636\n",
            "1    486\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"Class distribution:\\n\", y.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8b538fc7",
      "metadata": {
        "id": "8b538fc7"
      },
      "outputs": [],
      "source": [
        "X = np.array(X)\n",
        "\n",
        "y = np.array(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "526216fc",
      "metadata": {
        "id": "526216fc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "88cebbb3",
      "metadata": {
        "id": "88cebbb3"
      },
      "outputs": [],
      "source": [
        "for i in range(len(X)):\n",
        "    X[i] = str(X[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3eaf8598",
      "metadata": {
        "id": "3eaf8598",
        "outputId": "420602d0-fe95-49ea-a880-cdd03fbfc2c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(897, 897, 225, 225)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# split into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,  random_state=42)\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f1a60ffb",
      "metadata": {
        "id": "f1a60ffb",
        "outputId": "f7c77011-b10c-4aab-b256-b95dabd10e04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of na: hyp                 0\n",
            "ref                 0\n",
            "src                 0\n",
            "tgt                 0\n",
            "model               0\n",
            "task                0\n",
            "label               0\n",
            "p(Hallucination)    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# count number of none 0 or 1s\n",
        "number_of_na = df.isna().sum()\n",
        "print(\"Number of na:\", number_of_na)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "bd5bfcd2",
      "metadata": {
        "id": "bd5bfcd2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "8326f8a1",
      "metadata": {
        "id": "8326f8a1",
        "outputId": "44844017-aaa5-4ff6-c1c2-30e6a15e613b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>src</th>\n",
              "      <th>tgt</th>\n",
              "      <th>hyp</th>\n",
              "      <th>task</th>\n",
              "      <th>labels</th>\n",
              "      <th>label</th>\n",
              "      <th>p(Hallucination)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>ты удивишься если я скажу что на самом деле ме...</td>\n",
              "      <td>would surprised told name isnt actually tom</td>\n",
              "      <td>youre gonna surprised say real name isnt tom</td>\n",
              "      <td>MT</td>\n",
              "      <td>[Not Hallucination, Not Hallucination, Not Hal...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>еды будет полно</td>\n",
              "      <td>plenty food</td>\n",
              "      <td>food full</td>\n",
              "      <td>MT</td>\n",
              "      <td>[Hallucination, Not Hallucination, Hallucinati...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>думаете том будет меня ждать</td>\n",
              "      <td>think tom wait</td>\n",
              "      <td>think toms gonna wait</td>\n",
              "      <td>MT</td>\n",
              "      <td>[Not Hallucination, Not Hallucination, Not Hal...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>два брата довольно разные</td>\n",
              "      <td>two brothers pretty different</td>\n",
              "      <td>theres lot friends</td>\n",
              "      <td>MT</td>\n",
              "      <td>[Hallucination, Hallucination, Hallucination, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>infradiaphragmatic intra suprasellar craniopha...</td>\n",
              "      <td>medicine diaphragm</td>\n",
              "      <td>anatomy relating diaphragm</td>\n",
              "      <td>DM</td>\n",
              "      <td>[Hallucination, Hallucination, Hallucination, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1495</th>\n",
              "      <td>2992</td>\n",
              "      <td>я никогда не говорил мэри что чувствую</td>\n",
              "      <td>never told mary felt</td>\n",
              "      <td>ive never told mary feel</td>\n",
              "      <td>MT</td>\n",
              "      <td>[Hallucination, Not Hallucination, Not Halluci...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1496</th>\n",
              "      <td>2993</td>\n",
              "      <td>beat rat tailed kyoodle runs steers eric laid ...</td>\n",
              "      <td>mutt dog mixed breed little value noisy dog</td>\n",
              "      <td>slang mustang</td>\n",
              "      <td>DM</td>\n",
              "      <td>[Hallucination, Hallucination, Hallucination, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1497</th>\n",
              "      <td>2994</td>\n",
              "      <td>ты знаешь почему они прекратили говорить</td>\n",
              "      <td>know stopped talking</td>\n",
              "      <td>know stopped talking</td>\n",
              "      <td>MT</td>\n",
              "      <td>[Hallucination, Not Hallucination, Not Halluci...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1498</th>\n",
              "      <td>2996</td>\n",
              "      <td>anyone back</td>\n",
              "      <td>anyone confirm</td>\n",
              "      <td>anyone corroborate</td>\n",
              "      <td>PG</td>\n",
              "      <td>[Not Hallucination, Not Hallucination, Not Hal...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1499</th>\n",
              "      <td>2999</td>\n",
              "      <td>oh michael youre scaring</td>\n",
              "      <td>frighten</td>\n",
              "      <td>oh michael scare</td>\n",
              "      <td>PG</td>\n",
              "      <td>[Not Hallucination, Not Hallucination, Not Hal...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1500 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                                src  \\\n",
              "0        1  ты удивишься если я скажу что на самом деле ме...   \n",
              "1        2                                    еды будет полно   \n",
              "2        3                       думаете том будет меня ждать   \n",
              "3        6                          два брата довольно разные   \n",
              "4        7  infradiaphragmatic intra suprasellar craniopha...   \n",
              "...    ...                                                ...   \n",
              "1495  2992             я никогда не говорил мэри что чувствую   \n",
              "1496  2993  beat rat tailed kyoodle runs steers eric laid ...   \n",
              "1497  2994           ты знаешь почему они прекратили говорить   \n",
              "1498  2996                                        anyone back   \n",
              "1499  2999                           oh michael youre scaring   \n",
              "\n",
              "                                              tgt  \\\n",
              "0     would surprised told name isnt actually tom   \n",
              "1                                     plenty food   \n",
              "2                                  think tom wait   \n",
              "3                   two brothers pretty different   \n",
              "4                              medicine diaphragm   \n",
              "...                                           ...   \n",
              "1495                         never told mary felt   \n",
              "1496  mutt dog mixed breed little value noisy dog   \n",
              "1497                         know stopped talking   \n",
              "1498                               anyone confirm   \n",
              "1499                                     frighten   \n",
              "\n",
              "                                               hyp task  \\\n",
              "0     youre gonna surprised say real name isnt tom   MT   \n",
              "1                                        food full   MT   \n",
              "2                            think toms gonna wait   MT   \n",
              "3                               theres lot friends   MT   \n",
              "4                       anatomy relating diaphragm   DM   \n",
              "...                                            ...  ...   \n",
              "1495                      ive never told mary feel   MT   \n",
              "1496                                 slang mustang   DM   \n",
              "1497                          know stopped talking   MT   \n",
              "1498                            anyone corroborate   PG   \n",
              "1499                              oh michael scare   PG   \n",
              "\n",
              "                                                 labels  label  \\\n",
              "0     [Not Hallucination, Not Hallucination, Not Hal...      0   \n",
              "1     [Hallucination, Not Hallucination, Hallucinati...      1   \n",
              "2     [Not Hallucination, Not Hallucination, Not Hal...      0   \n",
              "3     [Hallucination, Hallucination, Hallucination, ...      1   \n",
              "4     [Hallucination, Hallucination, Hallucination, ...      1   \n",
              "...                                                 ...    ...   \n",
              "1495  [Hallucination, Not Hallucination, Not Halluci...      0   \n",
              "1496  [Hallucination, Hallucination, Hallucination, ...      1   \n",
              "1497  [Hallucination, Not Hallucination, Not Halluci...      0   \n",
              "1498  [Not Hallucination, Not Hallucination, Not Hal...      0   \n",
              "1499  [Not Hallucination, Not Hallucination, Not Hal...      0   \n",
              "\n",
              "      p(Hallucination)  \n",
              "0                  0.0  \n",
              "1                  0.8  \n",
              "2                  0.2  \n",
              "3                  1.0  \n",
              "4                  0.8  \n",
              "...                ...  \n",
              "1495               0.4  \n",
              "1496               1.0  \n",
              "1497               0.4  \n",
              "1498               0.0  \n",
              "1499               0.0  \n",
              "\n",
              "[1500 rows x 8 columns]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# paths to final test data\n",
        "final_ag_path = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/final_test_data/test.model-agnostic.json'\n",
        "final_aw_path = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/final_test_data/test.model-agnostic.json'\n",
        "\n",
        "final_ag_df = pd.read_json(final_ag_path)\n",
        "final_aw_df = pd.read_json(final_aw_path)\n",
        "final_ag_df[\"label\"] = final_ag_df[\"label\"].map({\"Hallucination\":1, \"Not Hallucination\":0})\n",
        "final_aw_df[\"label\"] = final_aw_df[\"label\"].map({\"Hallucination\":1, \"Not Hallucination\":0})\n",
        "# perform preprocessing on final_df AND split it based on tasks!\n",
        "columns = ['hyp','src','tgt'] # exclude task\n",
        "for x in columns:\n",
        "    final_ag_df[x] = final_ag_df[x].apply(preprocess_data)\n",
        "for x in columns:\n",
        "    final_aw_df[x] = final_aw_df[x].apply(preprocess_data)\n",
        "final_ag_pg = final_ag_df[final_ag_df['task'] == 'PG']\n",
        "final_ag_dm = final_ag_df[final_ag_df['task'] == 'DM']\n",
        "final_ag_mt = final_ag_df[final_ag_df['task'] == 'MT']\n",
        "\n",
        "final_aw_pg = final_aw_df[final_aw_df['task'] == 'PG']\n",
        "final_aw_dm = final_aw_df[final_aw_df['task'] == 'DM']\n",
        "final_aw_mt = final_aw_df[final_aw_df['task'] == 'MT']\n",
        "\n",
        "final_ag_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "cc9b9eaf",
      "metadata": {
        "id": "cc9b9eaf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "30778a48",
      "metadata": {
        "id": "30778a48",
        "outputId": "301e0fa4-afb7-4c28-a681-eb8383703525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\marko\\miniconda3\\envs\\tf\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import TFBertForSequenceClassification, BertTokenizer, AutoTokenizer, TFAutoModel\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# two other models i wanna try is BERT-large and RoBERTa\n",
        "checkpoint = 'bert-base-uncased'\n",
        "# checkpoint = 'bert-large-uncased'\n",
        "# Load BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(checkpoint)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "1d37f260",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.16.1\n",
            "4.41.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import transformers\n",
        "print(tf.__version__)\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "0fdd17eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ['TF_USE_LEGACY_KERAS'] = '1' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "722485b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tf_keras\n",
        "adam = tf_keras.src.optimizers.Adam(learning_rate=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "a2479b6c",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            ""
          ]
        },
        "id": "a2479b6c",
        "outputId": "9620ff12-cba4-476b-ac57-572e38226df6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\marko\\miniconda3\\envs\\tf\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Map:   0%|          | 0/897 [00:00<?, ? examples/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Map:  25%|██▍       | 224/897 [00:00<00:00, 2153.01 examples/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Map:  52%|█████▏    | 464/897 [00:00<00:00, 2206.63 examples/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Map:  89%|████████▉ | 800/897 [00:00<00:00, 2125.56 examples/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Map: 100%|██████████| 897/897 [00:00<00:00, 2138.77 examples/s]\n",
            "Map:   0%|          | 0/225 [00:00<?, ? examples/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Map: 100%|██████████| 225/225 [00:00<00:00, 2019.39 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x00000222244E00E0> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function infer_framework at 0x00000222244E00E0> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:From c:\\Users\\marko\\miniconda3\\envs\\tf\\Lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\marko\\miniconda3\\envs\\tf\\Lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\marko\\miniconda3\\envs\\tf\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "  3/113 [..............................] - ETA: 2:33 - loss: 0.6974 - accuracy: 0.7083 "
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[29], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     86\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 87\u001b[0m \u001b[43mbert_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     89\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(end \u001b[38;5;241m-\u001b[39m start, \u001b[38;5;241m2\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\marko\\miniconda3\\envs\\tf\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:1229\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1228\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\marko\\miniconda3\\envs\\tf\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\marko\\miniconda3\\envs\\tf\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py:1804\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1796\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1798\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1801\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1802\u001b[0m ):\n\u001b[0;32m   1803\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1804\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1806\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[1;32mc:\\Users\\marko\\miniconda3\\envs\\tf\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\marko\\miniconda3\\envs\\tf\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\marko\\miniconda3\\envs\\tf\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[1;32mc:\\Users\\marko\\miniconda3\\envs\\tf\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\marko\\miniconda3\\envs\\tf\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\marko\\miniconda3\\envs\\tf\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[1;32mc:\\Users\\marko\\miniconda3\\envs\\tf\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\marko\\miniconda3\\envs\\tf\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
            "File \u001b[1;32mc:\\Users\\marko\\miniconda3\\envs\\tf\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# this here uses manual cross validation!\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import datasets\n",
        "import time\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 5\n",
        "N_SPLITS = 5\n",
        "\n",
        "# Shuffle and split data into N_SPLITS folds\n",
        "indices = np.arange(len(X))\n",
        "np.random.shuffle(indices)\n",
        "folds = np.array_split(indices, N_SPLITS)\n",
        "\n",
        "# Initialize lists to store evaluation metrics\n",
        "all_val_preds = []\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1s = []\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example['hyp'], text_pair=example['tgt'], truncation=True, padding='max_length', max_length=100)\n",
        "\n",
        "def order(inp):\n",
        "    data = list(inp.values())\n",
        "    return {\n",
        "        'input_ids': tf.convert_to_tensor(data[1], dtype=tf.int32),\n",
        "        'token_type_ids': tf.convert_to_tensor(data[2], dtype=tf.int32),\n",
        "        'attention_mask': tf.convert_to_tensor(data[3], dtype=tf.int32),\n",
        "    }, data[0]\n",
        "# changed from optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-5), to without legacy\n",
        "# bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=1)\n",
        "# bert_model.compile(\n",
        "#     # optimizer=Adam,\n",
        "#     loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "#     metrics=['accuracy']\n",
        "# )\n",
        "\n",
        "for i in range(N_SPLITS):\n",
        "    # Reset model for each fold\n",
        "    bert_model = TFBertForSequenceClassification.from_pretrained(checkpoint, num_labels=1)\n",
        "    \n",
        "    # Compile the model with appropriate loss function and optimizer\n",
        "    bert_model.compile(\n",
        "        # first try out lr of 1e-6, then 1e-5, then 1e-4\n",
        "        # optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-6),\n",
        "        optimizer=adam,\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    # the code below resets model, and I might try it out? maybe idk\n",
        "    # Prepare training and validation indices\n",
        "    val_indices = folds[i]\n",
        "    train_indices = np.hstack([folds[j] for j in range(N_SPLITS) if j != i])\n",
        "\n",
        "    # Split the data\n",
        "    X_train2, X_val = X[train_indices], X[val_indices]\n",
        "    y_train2, y_val = y[train_indices], y[val_indices]\n",
        "    # Create Pandas DataFrames\n",
        "    X_train2_df = pd.DataFrame({'hyp': X_train2[:,0], 'tgt':X_train2[:,1], 'label': y_train2})\n",
        "    X_val_df = pd.DataFrame({'hyp': X_val[:,0], 'tgt': X_val[:,1], 'label': y_val})\n",
        "\n",
        "    # Convert to HuggingFace Datasets\n",
        "    df_train = datasets.Dataset.from_pandas(X_train2_df)\n",
        "    df_val = datasets.Dataset.from_pandas(X_val_df)\n",
        "    dataset = datasets.DatasetDict({'train': df_train, 'val': df_val})\n",
        "\n",
        "    # Tokenize the datasets\n",
        "    tokenized_datasets_mapped = dataset.map(tokenize_function, batched=True, batch_size=BATCH_SIZE, remove_columns=['hyp','tgt'])\n",
        "\n",
        "    # Convert to TensorFlow Datasets\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(tokenized_datasets_mapped['train'][:])\n",
        "    train_dataset = train_dataset.batch(BATCH_SIZE).shuffle(1000)\n",
        "    train_dataset = train_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices(tokenized_datasets_mapped['val'][:])\n",
        "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
        "    val_dataset = val_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # Train the model\n",
        "    start = time.time()\n",
        "    bert_model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS)\n",
        "    end = time.time()\n",
        "    elapsed_time = round(end - start, 2)\n",
        "    print(f'Fitting fold {i+1} is DONE!! Time: {elapsed_time}s')\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluated = bert_model.evaluate(val_dataset)\n",
        "    val_preds = bert_model.predict(val_dataset).logits\n",
        "#     val_preds = tf.nn.softmax(val_preds, axis=1).numpy()[:, 1]  # Probability of class 1, 1 try\n",
        "    # val_preds = tf.nn.sigmoid(val_preds).numpy().squeeze()[:, 1] # 2nd attempt, also i might need try out\n",
        "    # using a [:, 0] instead?\n",
        "    # Apply sigmoid activation\n",
        "    val_preds = tf.nn.sigmoid(val_preds).numpy()\n",
        "\n",
        "    # Check if val_preds has more than one dimension\n",
        "    if val_preds.ndim > 1:\n",
        "        val_preds = val_preds.squeeze()\n",
        "        # Ensure that there are at least two dimensions to index\n",
        "        if val_preds.ndim > 1:\n",
        "            val_preds = val_preds[:, 1]\n",
        "        else:\n",
        "            # Handle case where squeezing resulted in a single dimension\n",
        "            val_preds = val_preds.flatten()\n",
        "    else:\n",
        "        val_preds = val_preds.flatten()\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_val, val_preds.round(), average='weighted')\n",
        "    acc = accuracy_score(y_val, val_preds.round())\n",
        "\n",
        "    all_val_preds.append(val_preds)\n",
        "    accuracies.append(acc)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1s.append(f1)\n",
        "\n",
        "# Calculate and print average metrics\n",
        "print(f'Mean Accuracy: {np.mean(accuracies)}')\n",
        "print(f'Mean Precision: {np.mean(precisions)}')\n",
        "print(f'Mean Recall: {np.mean(recalls)}')\n",
        "print(f'Mean F1 Score: {np.mean(f1s)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5da4f0f2",
      "metadata": {
        "id": "5da4f0f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 562/562 [00:00<00:00, 3447.44 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Convert to TensorFlow Datasets\n",
        "def order_final(example):\n",
        "    return {\n",
        "        'input_ids': tf.convert_to_tensor(example['input_ids'], dtype=tf.int32),\n",
        "        'attention_mask': tf.convert_to_tensor(example['attention_mask'], dtype=tf.int32),\n",
        "        'token_type_ids': tf.convert_to_tensor(example['token_type_ids'], dtype=tf.int32),\n",
        "    }, tf.convert_to_tensor(example['label'], dtype=tf.int32)\n",
        "\n",
        "\n",
        "# final two datasets\n",
        "ag_X, ag_y = final_ag_pg[['hyp','tgt']], final_ag_pg['label']\n",
        "# ag_X, ag_y = final_ag_dm[['hyp','tgt']], final_ag_dm['label']\n",
        "# ag_X, ag_y = final_ag_mt[['hyp','tgt']], final_ag_mt['label']\n",
        "\n",
        "# ag_X, ag_y = final_aw_pg['hyp'], final_aw_pg['label']\n",
        "# ag_X, ag_y = final_aw_dm['hyp'], final_aw_dm['label']\n",
        "# ag_X, ag_y = final_aw_mt['hyp'], final_aw_mt['label']\n",
        "\n",
        "# Create Pandas DataFrames\n",
        "final_df = pd.DataFrame({'hyp': ag_X['hyp'], 'tgt': ag_X['tgt'], 'label': ag_y})\n",
        "\n",
        "# Convert to HuggingFace Datasets\n",
        "final_df2 = datasets.Dataset.from_pandas(final_df)\n",
        "dataset = datasets.DatasetDict({'final': final_df2})\n",
        "\n",
        "# Tokenize the datasets\n",
        "tokenized_datasets_mapped = dataset.map(tokenize_function, batched=True, batch_size=BATCH_SIZE, remove_columns=['hyp','tgt'])\n",
        "\n",
        "# Convert to TensorFlow Datasets\n",
        "final_dataset = tf.data.Dataset.from_tensor_slices(tokenized_datasets_mapped['final'][:])\n",
        "final_dataset = final_dataset.batch(BATCH_SIZE)\n",
        "final_dataset = final_dataset.map(order_final, num_parallel_calls=tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f62e4d",
      "metadata": {
        "id": "94f62e4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "71/71 [==============================] - 43s 599ms/step\n"
          ]
        }
      ],
      "source": [
        "# my attempt failed for final dataset\n",
        "# evaluated = model.evaluate(final_dataset)\n",
        "final_preds = bert_model.predict(final_dataset).logits\n",
        "final_preds = tf.nn.sigmoid(final_preds).numpy()\n",
        "# Check if val_preds has more than one dimension\n",
        "if final_preds.ndim > 1:\n",
        "    final_preds = final_preds.squeeze()\n",
        "    # Ensure that there are at least two dimensions to index\n",
        "    if final_preds.ndim > 1:\n",
        "        final_preds = final_preds[:, 1]\n",
        "    else:\n",
        "        # Handle case where squeezing resulted in a single dimension\n",
        "        final_preds = final_preds.flatten()\n",
        "else:\n",
        "    final_preds = final_preds.flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "888e2def",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final aggregate accuracy BERT: 0.43238434163701067\n"
          ]
        }
      ],
      "source": [
        "final_preds_binary = [1 if x > 0.5 else 0 for x in final_preds]\n",
        "# final_preds_mean = np.mean(final_preds, axis=0)\n",
        "# final_labels = (final_preds_mean >= 0.5).astype(int)\n",
        "print(f'final aggregate accuracy BERT: {accuracy_score(final_preds_binary, ag_y)}')\n",
        "\n",
        "# train = pg_rd,final_val = ag_pg: 0.6533333333333333\n",
        "# train = pg_rd,final_val = ag_dm: 0.517\n",
        "# train = pg_rd,final_val = ag_mt: 0.537\n",
        "# jsut using 0s = 0.598\n",
        "\n",
        "# train = pg_sr, final_val = ag_pg: 0.675\n",
        "# train = pg_sr, final_val = ag_dm: 0.506\n",
        "# train = pg_sr, final_val = ag_mt: 0.56\n",
        "\n",
        "# train = pg_rs, final_val = ag_pg: 0.\n",
        "# train = pg_rs, final_val = ag_dm: 0.\n",
        "# train = pg_rs, final_val = ag_mt: 0.\n",
        "\n",
        "# using both hyp and tgt, \n",
        "# train = pg_sr, final_val = ag_pg: 0.681\n",
        "# train = pg_sr, final_val = ag_dm: 0.430\n",
        "# train = pg_sr, final_val = ag_mt: 0.500\n",
        "\n",
        "# train = dm_sr, final_val = ag_pg: 0.267\n",
        "# train = dm_sr, final_val = ag_dm: 0.575\n",
        "# train = dm_sr, final_val = ag_mt: 0.432\n",
        "\n",
        "# train = mt_sr, final_val = ag_pg: 0.267\n",
        "# train = mt_sr, final_val = ag_dm: 0.575\n",
        "# train = mt_sr, final_val = ag_mt: 0.432"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e99625c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "all zeros BERT: 0.597864768683274\n"
          ]
        }
      ],
      "source": [
        "all_zeros = [0 for x in final_preds]\n",
        "print(f'all zeros BERT: {accuracy_score(all_zeros, ag_y)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e47d5cf",
      "metadata": {
        "id": "1e47d5cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label\n",
            "0    336\n",
            "1    226\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(ag_y.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4476a729",
      "metadata": {
        "id": "4476a729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8427\n",
            "Precision: 0.8756\n",
            "Recall: 0.8427\n",
            "F1 Score: 0.8171\n",
            "Final Predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0]\n"
          ]
        }
      ],
      "source": [
        "# i guess right now the big question is, is there a point in using cross validation?\n",
        "# unless to get best hyper paramter, which i believe i can use some other library for that,\n",
        "# what is the point? chatgpt said training final model invovles just using only\n",
        "# train test split so...\n",
        "final_preds = np.mean(all_val_preds, axis=0)\n",
        "final_labels = (final_preds >= 0.5).astype(int)\n",
        "\n",
        "print(f'Accuracy: {np.mean(accuracies):.4f}')\n",
        "print(f'Precision: {np.mean(precisions):.4f}')\n",
        "print(f'Recall: {np.mean(recalls):.4f}')\n",
        "print(f'F1 Score: {np.mean(f1s):.4f}')\n",
        "print(f'Final Predictions: {final_labels}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "033d55f1",
      "metadata": {
        "id": "033d55f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 375/375 [00:00<00:00, 1317.01 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47/47 [==============================] - 61s 1s/step\n",
            "Final aggregate accuracy BERT: 0.6426666666666667\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score\n",
        "import datasets\n",
        "\n",
        "# Ensure eager execution\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "# Create Pandas DataFrame without the index\n",
        "final_df = pd.DataFrame({'hyp': ag_X, 'label': ag_y})\n",
        "\n",
        "# Convert to HuggingFace Datasets\n",
        "final_df2 = datasets.Dataset.from_pandas(final_df)\n",
        "dataset = datasets.DatasetDict({'final': final_df2})\n",
        "\n",
        "# Initialize tokenizer (make sure you have the right model name)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the datasets\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example['hyp'], truncation=True, padding='max_length', max_length=100)\n",
        "\n",
        "tokenized_datasets_mapped = dataset.map(tokenize_function, batched=True, batch_size=BATCH_SIZE, remove_columns=['hyp','tgt'])\n",
        "\n",
        "# Use from_generator to create dataset with correct output signature\n",
        "# final_dataset = tf.data.Dataset.from_generator(\n",
        "#     lambda: iter(tokenized_datasets_mapped['final']),\n",
        "#     output_signature=(\n",
        "#         {\n",
        "#             'input_ids': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "#             'attention_mask': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "#             'token_type_ids': tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
        "#         },\n",
        "#         tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "#     )\n",
        "# )\n",
        "final_dataset = tf.data.Dataset.from_tensor_slices(tokenized_datasets_mapped['final'][:])\n",
        "final_dataset = final_dataset.batch(BATCH_SIZE)\n",
        "final_dataset = final_dataset.map(order_final, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Evaluate the model\n",
        "final_preds = bert_model.predict(final_dataset).logits\n",
        "\n",
        "# Apply sigmoid to get probabilities and flatten the tensor\n",
        "final_preds = tf.nn.sigmoid(final_preds).numpy().squeeze()\n",
        "\n",
        "# If final_preds is still multidimensional, reduce it to a 1D array\n",
        "if final_preds.ndim > 1:\n",
        "    final_preds = final_preds[:, 0]\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(ag_y, final_preds.round())\n",
        "print(f'Final aggregate accuracy BERT: {accuracy}')\n",
        "\n",
        "# keep track of results here for now\n",
        "# for task pg, 0.645\n",
        "# for task dm, 0.453\n",
        "# for task mt, 0.486"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf2983cc",
      "metadata": {
        "id": "cf2983cc"
      },
      "outputs": [],
      "source": [
        "# import json\n",
        "# copy_of_predictions = list(predictions)\n",
        "# index = 0\n",
        "# for i in range(len(copy_of_predictions)):\n",
        "#     copy_of_predictions[i] = list(dict(copy_of_predictions[i]).values())\n",
        "# #     for key,value in copy_of_predictions[i].items():\n",
        "# #         copy_of_predictions[i][key] = list(value)\n",
        "\n",
        "# print(copy_of_predictions)\n",
        "# with open('mt_sr_160.json','w') as f:\n",
        "# #     f.write(copy_of_predictions)\n",
        "#     json.dump(copy_of_predictions, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d24cd4c",
      "metadata": {
        "id": "2d24cd4c"
      },
      "outputs": [],
      "source": [
        "# kf.get_n_splits(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "658504c4",
      "metadata": {
        "id": "658504c4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch_cuda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
