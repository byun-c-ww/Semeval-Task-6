{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "aa9bceb0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import seaborn as sns\n",
        "import string\n",
        "import re\n",
        "# sns.set()\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Graphics in retina format are more sharp and legible\n",
        "%config InlineBackend.figure_format = 'retina'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "b521f9dc",
      "metadata": {
        "id": "b521f9dc"
      },
      "outputs": [],
      "source": [
        "# C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/thesis_data_after\n",
        "# task PG, num_aug 16\n",
        "\n",
        "# just use sr, and rs 6/6\n",
        "pg_rd_path = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/rd/pg_df_rd.csv'\n",
        "pg_ri_path = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/ri/pg_df_ri.csv'\n",
        "pg_rs_path = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_lemmatized_after/rs/pg_df_rs.csv'\n",
        "pg_sr_path = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_lemmatized_after/sr/pg_df_sr.csv'\n",
        "\n",
        "pg_rd = pd.read_csv(pg_rd_path, encoding_errors='backslashreplace')\n",
        "pg_ri = pd.read_csv(pg_ri_path, encoding_errors='backslashreplace')\n",
        "pg_rs = pd.read_csv(pg_rs_path, encoding_errors='backslashreplace')\n",
        "pg_sr = pd.read_csv(pg_sr_path, encoding_errors='backslashreplace')\n",
        "\n",
        "# num_aug 160\n",
        "path1 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/rd/pg_df_rd.csv'\n",
        "path2 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/ri/pg_df_ri.csv'\n",
        "path3 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/rs/pg_df_rs.csv'\n",
        "path4 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/sr/pg_df_sr.csv'\n",
        "\n",
        "pg_rd_160 = pd.read_csv(path1, encoding_errors='backslashreplace')\n",
        "pg_ri_160 = pd.read_csv(path2, encoding_errors='backslashreplace')\n",
        "pg_rs_160 = pd.read_csv(path3, encoding_errors='backslashreplace')\n",
        "pg_sr_160 = pd.read_csv(path4, encoding_errors='backslashreplace')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "8033caa1",
      "metadata": {
        "id": "8033caa1"
      },
      "outputs": [],
      "source": [
        "# task DM 16\n",
        "path1 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/rd/dm_df_rd.csv'\n",
        "path2 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/ri/dm_df_ri.csv'\n",
        "path3 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_lemmatized_after/rs/dm_df_rs.csv'\n",
        "path4 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_lemmatized_after/sr/dm_df_sr.csv'\n",
        "\n",
        "dm_rd = pd.read_csv(path1, encoding_errors='backslashreplace')\n",
        "dm_ri = pd.read_csv(path2, encoding_errors='backslashreplace')\n",
        "dm_rs = pd.read_csv(path3, encoding_errors='backslashreplace')\n",
        "dm_sr = pd.read_csv(path4, encoding_errors='backslashreplace')\n",
        "\n",
        "# 160\n",
        "path1 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after_160/rd/dm_df_rd.csv'\n",
        "path2 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after_160/ri/dm_df_ri.csv'\n",
        "path3 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after_160/rs/dm_df_rs.csv'\n",
        "path4 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after_160/sr/dm_df_sr.csv'\n",
        "\n",
        "dm_rd_160 = pd.read_csv(path1, encoding_errors='backslashreplace')\n",
        "dm_ri_160 = pd.read_csv(path2, encoding_errors='backslashreplace')\n",
        "dm_rs_160 = pd.read_csv(path3, encoding_errors='backslashreplace')\n",
        "dm_sr_160 = pd.read_csv(path4, encoding_errors='backslashreplace')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "9eea0f24",
      "metadata": {
        "id": "9eea0f24"
      },
      "outputs": [],
      "source": [
        "# task MT 16\n",
        "path1 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/rd/mt_df_rd.csv'\n",
        "path2 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after/ri/mt_df_ri.csv'\n",
        "path3 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_lemmatized_after/rs/mt_df_rs.csv'\n",
        "path4 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_lemmatized_after/sr/mt_df_sr.csv'\n",
        "\n",
        "mt_rd = pd.read_csv(path1, encoding_errors='backslashreplace')\n",
        "mt_ri = pd.read_csv(path2, encoding_errors='backslashreplace')\n",
        "mt_rs = pd.read_csv(path3, encoding_errors='backslashreplace')\n",
        "mt_sr = pd.read_csv(path4, encoding_errors='backslashreplace')\n",
        "\n",
        "# 160\n",
        "path1 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after_160/rd/mt_df_rd.csv'\n",
        "path2 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after_160/ri/mt_df_ri.csv'\n",
        "path3 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after_160/rs/mt_df_rs.csv'\n",
        "path4 = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/eda_nlp-20240601T173748Z-001/eda_nlp/thesis_data_after_160/sr/mt_df_sr.csv'\n",
        "\n",
        "mt_rd_160 = pd.read_csv(path1, encoding_errors='backslashreplace')\n",
        "mt_ri_160 = pd.read_csv(path2, encoding_errors='backslashreplace')\n",
        "mt_rs_160 = pd.read_csv(path3, encoding_errors='backslashreplace')\n",
        "mt_sr_160 = pd.read_csv(path4, encoding_errors='backslashreplace')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "76d0a9d4",
      "metadata": {
        "id": "76d0a9d4",
        "outputId": "83d49d31-7a74-4252-9062-1dcf715159de"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hyp</th>\n",
              "      <th>ref</th>\n",
              "      <th>src</th>\n",
              "      <th>tgt</th>\n",
              "      <th>model</th>\n",
              "      <th>label</th>\n",
              "      <th>p(Hallucination)</th>\n",
              "      <th>Unnamed: 7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>john john one</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>john ann like</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>john john one</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>john ann like</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>john john one</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>john ann like</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>john one</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>john ann</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>john john one</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>john ann like</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7849</th>\n",
              "      <td>dont money buy dictionary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>money buy dictionary</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7850</th>\n",
              "      <td>dont money buy dictionary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>buy dictionary</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7851</th>\n",
              "      <td>dont money buy dictionary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>money buy dictionary</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7852</th>\n",
              "      <td>dont money buy dictionary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>money buy dictionary</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7853</th>\n",
              "      <td>dont money buy dictionary</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>money buy dictionary</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7854 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                            hyp     ref src                   tgt model  \\\n",
              "0                 john john one  either             john ann like         \n",
              "1                 john john one  either             john ann like         \n",
              "2                 john john one  either             john ann like         \n",
              "3                      john one  either                  john ann         \n",
              "4                 john john one  either             john ann like         \n",
              "...                         ...     ...  ..                   ...   ...   \n",
              "7849  dont money buy dictionary  either      money buy dictionary         \n",
              "7850  dont money buy dictionary  either            buy dictionary         \n",
              "7851  dont money buy dictionary  either      money buy dictionary         \n",
              "7852  dont money buy dictionary  either      money buy dictionary         \n",
              "7853  dont money buy dictionary  either      money buy dictionary         \n",
              "\n",
              "      label  p(Hallucination) Unnamed: 7  \n",
              "0         1               1.0             \n",
              "1         1               1.0             \n",
              "2         1               1.0             \n",
              "3         1               1.0             \n",
              "4         1               1.0             \n",
              "...     ...               ...        ...  \n",
              "7849      0               0.0             \n",
              "7850      0               0.0             \n",
              "7851      0               0.0             \n",
              "7852      0               0.0             \n",
              "7853      0               0.0             \n",
              "\n",
              "[7854 rows x 8 columns]"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# pg_rd = pg_rd.dropna(how=\"any\", axis=1)\n",
        "pg_rd = pg_rd.fillna('')\n",
        "pg_ri = pg_ri.fillna('')\n",
        "pg_rs = pg_rs.fillna('')\n",
        "pg_sr = pg_sr.fillna('')\n",
        "\n",
        "dm_rd = dm_rd.fillna('')\n",
        "dm_ri = dm_ri.fillna('')\n",
        "dm_rs = dm_rs.fillna('')\n",
        "dm_sr = dm_sr.fillna('')\n",
        "\n",
        "mt_rd = mt_rd.fillna('')\n",
        "mt_ri = mt_ri.fillna('')\n",
        "mt_rs = mt_rs.fillna('')\n",
        "mt_sr = mt_sr.fillna('')\n",
        "\n",
        "pg_rd_160 = pg_rd_160.fillna('')\n",
        "pg_ri_160 = pg_ri_160.fillna('')\n",
        "pg_rs_160 = pg_rs_160.fillna('')\n",
        "pg_sr_160 = pg_sr_160.fillna('')\n",
        "\n",
        "dm_rd_160 = dm_rd_160.fillna('')\n",
        "dm_ri_160 = dm_ri_160.fillna('')\n",
        "dm_rs_160 = dm_rs_160.fillna('')\n",
        "dm_sr_160 = dm_sr_160.fillna('')\n",
        "\n",
        "mt_rd_160 = mt_rd_160.fillna('')\n",
        "mt_ri_160 = mt_ri_160.fillna('')\n",
        "mt_rs_160 = mt_rs_160.fillna('')\n",
        "mt_sr_160 = mt_sr_160.fillna('')\n",
        "# drop na just deleted the model column i think\n",
        "mt_rd_160"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "8c9dc490",
      "metadata": {
        "id": "8c9dc490"
      },
      "outputs": [],
      "source": [
        "# ill just do lemmatize again and create new data augmentation dataset\n",
        "\n",
        "# columns = ['hyp','src','tgt'] # exclude task\n",
        "# def lemmatize_data(text):\n",
        "#     text = ' '.join(stemmer.stem(word) for word in text.split())                # Stemm all the words in the sentence\n",
        "#     return text\n",
        "# for x in columns:\n",
        "#     pg_sr[x] = pg_sr[x].apply(lemmatize_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "89942019",
      "metadata": {
        "id": "89942019"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hyp</th>\n",
              "      <th>ref</th>\n",
              "      <th>src</th>\n",
              "      <th>tgt</th>\n",
              "      <th>model</th>\n",
              "      <th>label</th>\n",
              "      <th>p(Hallucination)</th>\n",
              "      <th>Unnamed: 7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>course time number aristocratic power overran ...</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>thousand year agone man called aristarchus sai...</td>\n",
              "      <td>facebook/nllb-200-distilled-600M</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>course time number aristocratic power overran ...</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>thousand year ago man called aristarchus said ...</td>\n",
              "      <td>facebook/nllb-200-distilled-600M</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>course time number aristocratical power overra...</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>thousand year ago human called aristarchus sai...</td>\n",
              "      <td>facebook/nllb-200-distilled-600M</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>course metre number aristocratic power overran...</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>thousand year ago man called aristarchus said ...</td>\n",
              "      <td>facebook/nllb-200-distilled-600M</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>course of study time number aristocratic power...</td>\n",
              "      <td>either</td>\n",
              "      <td></td>\n",
              "      <td>thousand year ago man called aristarchus said ...</td>\n",
              "      <td>facebook/nllb-200-distilled-600M</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1123</th>\n",
              "      <td>exception fact u regime able negotiate contrac...</td>\n",
              "      <td>either</td>\n",
              "      <td>ihandjika iyakwece hali kulihana iya kutunguis...</td>\n",
              "      <td>liberal literary criticism reconstruction effo...</td>\n",
              "      <td>facebook/nllb-200-distilled-600M</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1124</th>\n",
              "      <td>exception fact u government able negotiate con...</td>\n",
              "      <td>either</td>\n",
              "      <td>ihandjika iyakwece hali kulihana iya kutunguis...</td>\n",
              "      <td>liberal criticism reconstruction effort focuse...</td>\n",
              "      <td>facebook/nllb-200-distilled-600M</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1125</th>\n",
              "      <td>exception fact u government able talk terms co...</td>\n",
              "      <td>either</td>\n",
              "      <td>ihandjika iyakwece hali kulihana iya kutunguis...</td>\n",
              "      <td>liberal criticism reconstruction effort focuse...</td>\n",
              "      <td>facebook/nllb-200-distilled-600M</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1126</th>\n",
              "      <td>exception fact u government able negotiate sig...</td>\n",
              "      <td>either</td>\n",
              "      <td>ihandjika iyakwece hali kulihana iya kutunguis...</td>\n",
              "      <td>handsome criticism reconstruction effort focus...</td>\n",
              "      <td>facebook/nllb-200-distilled-600M</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1127</th>\n",
              "      <td>exception fact u government able negotiate con...</td>\n",
              "      <td>either</td>\n",
              "      <td>ihandjika iyakwece hali kulihana iya kutunguis...</td>\n",
              "      <td>liberal criticism reconstruction effort focuse...</td>\n",
              "      <td>facebook/nllb-200-distilled-600M</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1128 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    hyp     ref  \\\n",
              "0     course time number aristocratic power overran ...  either   \n",
              "1     course time number aristocratic power overran ...  either   \n",
              "2     course time number aristocratical power overra...  either   \n",
              "3     course metre number aristocratic power overran...  either   \n",
              "4     course of study time number aristocratic power...  either   \n",
              "...                                                 ...     ...   \n",
              "1123  exception fact u regime able negotiate contrac...  either   \n",
              "1124  exception fact u government able negotiate con...  either   \n",
              "1125  exception fact u government able talk terms co...  either   \n",
              "1126  exception fact u government able negotiate sig...  either   \n",
              "1127  exception fact u government able negotiate con...  either   \n",
              "\n",
              "                                                    src  \\\n",
              "0                                                         \n",
              "1                                                         \n",
              "2                                                         \n",
              "3                                                         \n",
              "4                                                         \n",
              "...                                                 ...   \n",
              "1123  ihandjika iyakwece hali kulihana iya kutunguis...   \n",
              "1124  ihandjika iyakwece hali kulihana iya kutunguis...   \n",
              "1125  ihandjika iyakwece hali kulihana iya kutunguis...   \n",
              "1126  ihandjika iyakwece hali kulihana iya kutunguis...   \n",
              "1127  ihandjika iyakwece hali kulihana iya kutunguis...   \n",
              "\n",
              "                                                    tgt  \\\n",
              "0     thousand year agone man called aristarchus sai...   \n",
              "1     thousand year ago man called aristarchus said ...   \n",
              "2     thousand year ago human called aristarchus sai...   \n",
              "3     thousand year ago man called aristarchus said ...   \n",
              "4     thousand year ago man called aristarchus said ...   \n",
              "...                                                 ...   \n",
              "1123  liberal literary criticism reconstruction effo...   \n",
              "1124  liberal criticism reconstruction effort focuse...   \n",
              "1125  liberal criticism reconstruction effort focuse...   \n",
              "1126  handsome criticism reconstruction effort focus...   \n",
              "1127  liberal criticism reconstruction effort focuse...   \n",
              "\n",
              "                                 model  label  p(Hallucination) Unnamed: 7  \n",
              "0     facebook/nllb-200-distilled-600M      1               1.0             \n",
              "1     facebook/nllb-200-distilled-600M      1               1.0             \n",
              "2     facebook/nllb-200-distilled-600M      1               1.0             \n",
              "3     facebook/nllb-200-distilled-600M      1               1.0             \n",
              "4     facebook/nllb-200-distilled-600M      1               1.0             \n",
              "...                                ...    ...               ...        ...  \n",
              "1123  facebook/nllb-200-distilled-600M      0               0.4             \n",
              "1124  facebook/nllb-200-distilled-600M      0               0.4             \n",
              "1125  facebook/nllb-200-distilled-600M      0               0.4             \n",
              "1126  facebook/nllb-200-distilled-600M      0               0.4             \n",
              "1127  facebook/nllb-200-distilled-600M      0               0.4             \n",
              "\n",
              "[1128 rows x 8 columns]"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mt_sr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "3aa8a4ce",
      "metadata": {
        "id": "3aa8a4ce"
      },
      "outputs": [],
      "source": [
        "# original dataset before data aug\n",
        "# X, y = pg_df['hyp'], pg_df['label']\n",
        "# X, y = dm_df['hyp'], dm_df['label']\n",
        "# X, y = mt_df['hyp'], mt_df['label']\n",
        "\n",
        "# task PG\n",
        "# X, y = pg_rd[['hyp','tgt']] , pg_rd['label']\n",
        "# X, y = pg_ri[['hyp','tgt']] , pg_ri['label']\n",
        "# X, y = pg_rs[['hyp','tgt']] , pg_rs['label']\n",
        "# X, y = pg_sr[['hyp','tgt']] , pg_sr['label']\n",
        "# task DM\n",
        "# X, y = dm_rd[['hyp','tgt']] , dm_rd['label']\n",
        "# X, y = dm_ri[['hyp','tgt']], dm_ri['label']\n",
        "# X, y = dm_rs[['hyp','tgt']] , dm_rs['label']\n",
        "# X, y = dm_sr[['hyp','tgt']] , dm_sr['label']\n",
        "# task MT\n",
        "# X, y = mt_rd[['hyp','tgt']] , mt_rd['label']\n",
        "# X, y = mt_ri[['hyp','tgt']] , mt_ri['label']\n",
        "# X, y = mt_rs[['hyp','tgt']] , mt_rs['label']\n",
        "X, y = mt_sr[['hyp','tgt']] , mt_sr['label']\n",
        "\n",
        "# X, y = df['hyp'], df['label']\n",
        "# one way of getting label where task is DM, df.loc[df['task'] == 'DM', 'label']\n",
        "# X, y = df[df['task'] == 'PG']['hyp'], df[df['task'] == 'PG']['label']\n",
        "\n",
        "# task PG\n",
        "# X, y = pg_rd_160['hyp'] , pg_rd_160['label']\n",
        "# X, y = pg_ri_160['hyp'] , pg_ri_160['label']\n",
        "# X, y = pg_rs_160['hyp'] , pg_rs_160['label']\n",
        "# X, y = pg_sr_160['hyp'] , pg_sr_160['label']\n",
        "# task DM\n",
        "# X, y = dm_rd_160['hyp'] , dm_rd_160['label']\n",
        "# X, y = dm_ri_160['hyp'] , dm_ri_160['label']\n",
        "# X, y = dm_rs_160['hyp'] , dm_rs_160['label']\n",
        "# X, y = dm_sr_160['hyp'] , dm_sr_160['label']\n",
        "# task MT\n",
        "# X, y = mt_rd_160['hyp'] , mt_rd_160['label']\n",
        "# X, y = mt_ri_160['hyp'] , mt_ri_160['label']\n",
        "# X, y = mt_rs_160['hyp'] , mt_rs_160['label']\n",
        "# X, y = mt_sr_160['hyp'] , mt_sr_160['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "dbffdf41",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      0\n",
              "1      0\n",
              "2      0\n",
              "3      0\n",
              "4      0\n",
              "      ..\n",
              "745    0\n",
              "746    0\n",
              "747    0\n",
              "748    0\n",
              "749    0\n",
              "Name: label, Length: 750, dtype: int64"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pg_sr['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "56ac155c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "example text url email example example com\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# stemmer = nltk.SnowballStemmer(\"english\")\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    '''\n",
        "    Make text lowercase, remove text in square brackets, remove links, remove punctuation,\n",
        "    and remove words containing numbers.\n",
        "    '''\n",
        "    text = str(text).lower()\n",
        "    \n",
        "    # Remove text in square brackets\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    \n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    \n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    \n",
        "    # Remove punctuation\n",
        "    text = re.sub(rf'[{re.escape(string.punctuation)}]', ' ', text)\n",
        "    \n",
        "    # Remove new lines\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    \n",
        "    # Remove words containing numbers\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def preprocess_data(text):\n",
        "    text = clean_text(text)  # Clean punctuation, URLs, and so on\n",
        "    \n",
        "    # Remove stopwords\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "    \n",
        "    # Lemmatize all the words in the sentence\n",
        "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "example_text = \"This is an example text with a URL: http://example.com and an email: example@example.com.\"\n",
        "cleaned_text = preprocess_data(example_text)\n",
        "print(cleaned_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "c1e4f35f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'side casket cover heavi black broadcloth velvet cap present deeply contrast rich surmount mean surmount interior decoration featur sit top someth'"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_preprocessed = preprocess_data(\"side casket cover heavi black broadcloth velvet cap present deeply contrast rich surmount mean surmount,interior decoration featur sit top someth\")\n",
        "example_preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "fa5e2fe1",
      "metadata": {
        "id": "fa5e2fe1",
        "outputId": "77ee464f-18ec-496c-e1a3-638ff5e05d93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class distribution:\n",
            " label\n",
            "0    582\n",
            "1    546\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"Class distribution:\\n\", y.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "8b538fc7",
      "metadata": {
        "id": "8b538fc7"
      },
      "outputs": [],
      "source": [
        "X = np.array(X)\n",
        "\n",
        "y = np.array(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "88cebbb3",
      "metadata": {
        "id": "88cebbb3"
      },
      "outputs": [],
      "source": [
        "for i in range(len(X)):\n",
        "    X[i] = str(X[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "3eaf8598",
      "metadata": {
        "id": "3eaf8598",
        "outputId": "420602d0-fe95-49ea-a880-cdd03fbfc2c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(902, 902, 226, 226)"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# split into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,  random_state=42)\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "bd5bfcd2",
      "metadata": {
        "id": "bd5bfcd2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "8326f8a1",
      "metadata": {
        "id": "8326f8a1",
        "outputId": "44844017-aaa5-4ff6-c1c2-30e6a15e613b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>src</th>\n",
              "      <th>tgt</th>\n",
              "      <th>hyp</th>\n",
              "      <th>task</th>\n",
              "      <th>labels</th>\n",
              "      <th>label</th>\n",
              "      <th>p(Hallucination)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>ты удивишься если я скажу что на самом деле ме...</td>\n",
              "      <td>would surprised told name actually tom</td>\n",
              "      <td>gonna surprised say real name tom</td>\n",
              "      <td>MT</td>\n",
              "      <td>[Not Hallucination, Not Hallucination, Not Hal...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>еды будет полно</td>\n",
              "      <td>plenty food</td>\n",
              "      <td>food full</td>\n",
              "      <td>MT</td>\n",
              "      <td>[Hallucination, Not Hallucination, Hallucinati...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>думаете том будет меня ждать</td>\n",
              "      <td>think tom wait</td>\n",
              "      <td>think tom gonna wait</td>\n",
              "      <td>MT</td>\n",
              "      <td>[Not Hallucination, Not Hallucination, Not Hal...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>два брата довольно разные</td>\n",
              "      <td>two brother pretty different</td>\n",
              "      <td>lot friend</td>\n",
              "      <td>MT</td>\n",
              "      <td>[Hallucination, Hallucination, Hallucination, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>infradiaphragmatic intra suprasellar craniopha...</td>\n",
              "      <td>medicine diaphragm</td>\n",
              "      <td>anatomy relating diaphragm</td>\n",
              "      <td>DM</td>\n",
              "      <td>[Hallucination, Hallucination, Hallucination, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1495</th>\n",
              "      <td>2992</td>\n",
              "      <td>я никогда не говорил мэри что чувствую</td>\n",
              "      <td>never told mary felt</td>\n",
              "      <td>never told mary feel</td>\n",
              "      <td>MT</td>\n",
              "      <td>[Hallucination, Not Hallucination, Not Halluci...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1496</th>\n",
              "      <td>2993</td>\n",
              "      <td>beat rat tailed kyoodle run steer eric laid ha...</td>\n",
              "      <td>mutt dog mixed breed little value noisy dog</td>\n",
              "      <td>slang mustang</td>\n",
              "      <td>DM</td>\n",
              "      <td>[Hallucination, Hallucination, Hallucination, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1497</th>\n",
              "      <td>2994</td>\n",
              "      <td>ты знаешь почему они прекратили говорить</td>\n",
              "      <td>know stopped talking</td>\n",
              "      <td>know stopped talking</td>\n",
              "      <td>MT</td>\n",
              "      <td>[Hallucination, Not Hallucination, Not Halluci...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1498</th>\n",
              "      <td>2996</td>\n",
              "      <td>anyone back</td>\n",
              "      <td>anyone confirm</td>\n",
              "      <td>anyone corroborate</td>\n",
              "      <td>PG</td>\n",
              "      <td>[Not Hallucination, Not Hallucination, Not Hal...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1499</th>\n",
              "      <td>2999</td>\n",
              "      <td>oh michael scaring</td>\n",
              "      <td>frighten</td>\n",
              "      <td>oh michael scare</td>\n",
              "      <td>PG</td>\n",
              "      <td>[Not Hallucination, Not Hallucination, Not Hal...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1500 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                                src  \\\n",
              "0        1  ты удивишься если я скажу что на самом деле ме...   \n",
              "1        2                                    еды будет полно   \n",
              "2        3                       думаете том будет меня ждать   \n",
              "3        6                          два брата довольно разные   \n",
              "4        7  infradiaphragmatic intra suprasellar craniopha...   \n",
              "...    ...                                                ...   \n",
              "1495  2992             я никогда не говорил мэри что чувствую   \n",
              "1496  2993  beat rat tailed kyoodle run steer eric laid ha...   \n",
              "1497  2994           ты знаешь почему они прекратили говорить   \n",
              "1498  2996                                        anyone back   \n",
              "1499  2999                                 oh michael scaring   \n",
              "\n",
              "                                              tgt  \\\n",
              "0          would surprised told name actually tom   \n",
              "1                                     plenty food   \n",
              "2                                  think tom wait   \n",
              "3                    two brother pretty different   \n",
              "4                              medicine diaphragm   \n",
              "...                                           ...   \n",
              "1495                         never told mary felt   \n",
              "1496  mutt dog mixed breed little value noisy dog   \n",
              "1497                         know stopped talking   \n",
              "1498                               anyone confirm   \n",
              "1499                                     frighten   \n",
              "\n",
              "                                    hyp task  \\\n",
              "0     gonna surprised say real name tom   MT   \n",
              "1                             food full   MT   \n",
              "2                  think tom gonna wait   MT   \n",
              "3                            lot friend   MT   \n",
              "4            anatomy relating diaphragm   DM   \n",
              "...                                 ...  ...   \n",
              "1495               never told mary feel   MT   \n",
              "1496                      slang mustang   DM   \n",
              "1497               know stopped talking   MT   \n",
              "1498                 anyone corroborate   PG   \n",
              "1499                   oh michael scare   PG   \n",
              "\n",
              "                                                 labels  label  \\\n",
              "0     [Not Hallucination, Not Hallucination, Not Hal...      0   \n",
              "1     [Hallucination, Not Hallucination, Hallucinati...      1   \n",
              "2     [Not Hallucination, Not Hallucination, Not Hal...      0   \n",
              "3     [Hallucination, Hallucination, Hallucination, ...      1   \n",
              "4     [Hallucination, Hallucination, Hallucination, ...      1   \n",
              "...                                                 ...    ...   \n",
              "1495  [Hallucination, Not Hallucination, Not Halluci...      0   \n",
              "1496  [Hallucination, Hallucination, Hallucination, ...      1   \n",
              "1497  [Hallucination, Not Hallucination, Not Halluci...      0   \n",
              "1498  [Not Hallucination, Not Hallucination, Not Hal...      0   \n",
              "1499  [Not Hallucination, Not Hallucination, Not Hal...      0   \n",
              "\n",
              "      p(Hallucination)  \n",
              "0                  0.0  \n",
              "1                  0.8  \n",
              "2                  0.2  \n",
              "3                  1.0  \n",
              "4                  0.8  \n",
              "...                ...  \n",
              "1495               0.4  \n",
              "1496               1.0  \n",
              "1497               0.4  \n",
              "1498               0.0  \n",
              "1499               0.0  \n",
              "\n",
              "[1500 rows x 8 columns]"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# paths to final test data\n",
        "final_ag_path = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/final_test_data/test.model-agnostic.json'\n",
        "final_aw_path = 'C:/Users/marko/OneDrive/바탕 화면/semeval/Task 6 - windows/final_test_data/test.model-agnostic.json'\n",
        "\n",
        "final_ag_df = pd.read_json(final_ag_path, encoding_errors='backslashreplace')\n",
        "final_aw_df = pd.read_json(final_aw_path, encoding_errors='backslashreplace')\n",
        "final_ag_df[\"label\"] = final_ag_df[\"label\"].map({\"Hallucination\":1, \"Not Hallucination\":0})\n",
        "final_aw_df[\"label\"] = final_aw_df[\"label\"].map({\"Hallucination\":1, \"Not Hallucination\":0})\n",
        "# perform preprocessing on final_df AND split it based on tasks!\n",
        "columns = ['hyp','src','tgt'] # exclude task\n",
        "for x in columns:\n",
        "    final_ag_df[x] = final_ag_df[x].apply(preprocess_data)\n",
        "for x in columns:\n",
        "    final_aw_df[x] = final_aw_df[x].apply(preprocess_data)\n",
        "final_ag_pg = final_ag_df[final_ag_df['task'] == 'PG']\n",
        "final_ag_dm = final_ag_df[final_ag_df['task'] == 'DM']\n",
        "final_ag_mt = final_ag_df[final_ag_df['task'] == 'MT']\n",
        "\n",
        "final_aw_pg = final_aw_df[final_aw_df['task'] == 'PG']\n",
        "final_aw_dm = final_aw_df[final_aw_df['task'] == 'DM']\n",
        "final_aw_mt = final_aw_df[final_aw_df['task'] == 'MT']\n",
        "\n",
        "final_ag_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "cc9b9eaf",
      "metadata": {
        "id": "cc9b9eaf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "c05c0c9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import TFRobertaForSequenceClassification, AutoTokenizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "checkpoint = 'FacebookAI/roberta-base'\n",
        "# checkpoint = 'FacebookAI/roberta-large'\n",
        "# another model\n",
        "# checkpoint = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "fd0066e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tf_keras\n",
        "adam = tf_keras.src.optimizers.Adam(learning_rate=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "a24dd2c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# # Ensure eager execution\n",
        "# tf.config.run_functions_eagerly(True)\n",
        "# russian = tf.convert_to_tensor(\"ты знаешь почему они прекратили говорить\", dtype=tf.int32)\n",
        "# russian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "a2479b6c",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            ""
          ]
        },
        "id": "a2479b6c",
        "outputId": "9620ff12-cba4-476b-ac57-572e38226df6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
            "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Map: 100%|██████████| 902/902 [00:00<00:00, 3827.91 examples/s]\n",
            "Map: 100%|██████████| 226/226 [00:00<00:00, 4012.04 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "113/113 [==============================] - 782s 7s/step - loss: 0.6950 - accuracy: 0.5177 - val_loss: 0.6938 - val_accuracy: 0.5088\n",
            "Epoch 2/8\n",
            "113/113 [==============================] - 745s 7s/step - loss: 0.6956 - accuracy: 0.5177 - val_loss: 0.6918 - val_accuracy: 0.5088\n",
            "Epoch 3/8\n",
            "113/113 [==============================] - 743s 7s/step - loss: 0.6921 - accuracy: 0.5177 - val_loss: 0.6877 - val_accuracy: 0.5088\n",
            "Epoch 4/8\n",
            "113/113 [==============================] - 770s 7s/step - loss: 0.6826 - accuracy: 0.5177 - val_loss: 0.6700 - val_accuracy: 0.5088\n",
            "Epoch 5/8\n",
            "113/113 [==============================] - 766s 7s/step - loss: 0.5814 - accuracy: 0.6020 - val_loss: 0.3870 - val_accuracy: 0.8938\n",
            "Epoch 6/8\n",
            "113/113 [==============================] - 773s 7s/step - loss: 0.3359 - accuracy: 0.8836 - val_loss: 0.1995 - val_accuracy: 0.9513\n",
            "Epoch 7/8\n",
            "113/113 [==============================] - 786s 7s/step - loss: 0.1883 - accuracy: 0.9545 - val_loss: 0.1582 - val_accuracy: 0.9425\n",
            "Epoch 8/8\n",
            "113/113 [==============================] - 824s 7s/step - loss: 0.1390 - accuracy: 0.9634 - val_loss: 0.1223 - val_accuracy: 0.9690\n",
            "Fitting fold 1 is DONE!! Time: 6189.87s\n",
            "29/29 [==============================] - 56s 2s/step - loss: 0.1223 - accuracy: 0.9690\n",
            "29/29 [==============================] - 56s 2s/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
            "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Map: 100%|██████████| 902/902 [00:00<00:00, 3398.10 examples/s]\n",
            "Map: 100%|██████████| 226/226 [00:00<00:00, 4318.75 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "113/113 [==============================] - 747s 7s/step - loss: 0.6943 - accuracy: 0.5144 - val_loss: 0.6898 - val_accuracy: 0.5221\n",
            "Epoch 2/8\n",
            "113/113 [==============================] - 742s 7s/step - loss: 0.6807 - accuracy: 0.5144 - val_loss: 0.6231 - val_accuracy: 0.5221\n",
            "Epoch 3/8\n",
            "113/113 [==============================] - 722s 6s/step - loss: 0.4051 - accuracy: 0.8126 - val_loss: 0.2411 - val_accuracy: 0.9248\n",
            "Epoch 4/8\n",
            "113/113 [==============================] - 715s 6s/step - loss: 0.1962 - accuracy: 0.9479 - val_loss: 0.1807 - val_accuracy: 0.9425\n",
            "Epoch 5/8\n",
            "113/113 [==============================] - 716s 6s/step - loss: 0.1272 - accuracy: 0.9667 - val_loss: 0.1038 - val_accuracy: 0.9690\n",
            "Epoch 6/8\n",
            "113/113 [==============================] - 761s 7s/step - loss: 0.0991 - accuracy: 0.9734 - val_loss: 0.0862 - val_accuracy: 0.9646\n",
            "Epoch 7/8\n",
            "113/113 [==============================] - 779s 7s/step - loss: 0.0708 - accuracy: 0.9823 - val_loss: 0.0540 - val_accuracy: 0.9867\n",
            "Epoch 8/8\n",
            "113/113 [==============================] - 773s 7s/step - loss: 0.0583 - accuracy: 0.9900 - val_loss: 0.0595 - val_accuracy: 0.9823\n",
            "Fitting fold 2 is DONE!! Time: 5955.12s\n",
            "29/29 [==============================] - 49s 2s/step - loss: 0.0595 - accuracy: 0.9823\n",
            "29/29 [==============================] - 49s 2s/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
            "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Map: 100%|██████████| 902/902 [00:00<00:00, 3612.84 examples/s]\n",
            "Map: 100%|██████████| 226/226 [00:00<00:00, 4821.14 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "113/113 [==============================] - 680s 6s/step - loss: 0.6972 - accuracy: 0.5211 - val_loss: 0.6875 - val_accuracy: 0.4912\n",
            "Epoch 2/8\n",
            "113/113 [==============================] - 681s 6s/step - loss: 0.6325 - accuracy: 0.5222 - val_loss: 0.5026 - val_accuracy: 0.4912\n",
            "Epoch 3/8\n",
            "113/113 [==============================] - 680s 6s/step - loss: 0.4139 - accuracy: 0.6707 - val_loss: 0.3082 - val_accuracy: 0.9027\n",
            "Epoch 4/8\n",
            "113/113 [==============================] - 680s 6s/step - loss: 0.2095 - accuracy: 0.9335 - val_loss: 0.1861 - val_accuracy: 0.9381\n",
            "Epoch 5/8\n",
            "113/113 [==============================] - 681s 6s/step - loss: 0.1395 - accuracy: 0.9590 - val_loss: 0.1543 - val_accuracy: 0.9425\n",
            "Epoch 6/8\n",
            "113/113 [==============================] - 700s 6s/step - loss: 0.0967 - accuracy: 0.9723 - val_loss: 0.1109 - val_accuracy: 0.9602\n",
            "Epoch 7/8\n",
            "113/113 [==============================] - 737s 7s/step - loss: 0.0818 - accuracy: 0.9756 - val_loss: 0.0939 - val_accuracy: 0.9602\n",
            "Epoch 8/8\n",
            "113/113 [==============================] - 774s 7s/step - loss: 0.0731 - accuracy: 0.9800 - val_loss: 0.0823 - val_accuracy: 0.9646\n",
            "Fitting fold 3 is DONE!! Time: 5613.37s\n",
            "29/29 [==============================] - 58s 2s/step - loss: 0.0823 - accuracy: 0.9646\n",
            "29/29 [==============================] - 58s 2s/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
            "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Map: 100%|██████████| 903/903 [00:00<00:00, 3477.89 examples/s]\n",
            "Map: 100%|██████████| 225/225 [00:00<00:00, 4642.39 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "113/113 [==============================] - 753s 7s/step - loss: 0.6883 - accuracy: 0.5127 - val_loss: 0.6682 - val_accuracy: 0.5244\n",
            "Epoch 2/8\n",
            "113/113 [==============================] - 718s 6s/step - loss: 0.4982 - accuracy: 0.7752 - val_loss: 0.3721 - val_accuracy: 0.8667\n",
            "Epoch 3/8\n",
            "113/113 [==============================] - 716s 6s/step - loss: 0.2623 - accuracy: 0.9192 - val_loss: 0.1963 - val_accuracy: 0.9333\n",
            "Epoch 4/8\n",
            "113/113 [==============================] - 717s 6s/step - loss: 0.1461 - accuracy: 0.9601 - val_loss: 0.1215 - val_accuracy: 0.9644\n",
            "Epoch 5/8\n",
            "113/113 [==============================] - 715s 6s/step - loss: 0.0718 - accuracy: 0.9878 - val_loss: 0.0937 - val_accuracy: 0.9644\n",
            "Epoch 6/8\n",
            "113/113 [==============================] - 714s 6s/step - loss: 0.0597 - accuracy: 0.9867 - val_loss: 0.0833 - val_accuracy: 0.9733\n",
            "Epoch 7/8\n",
            "113/113 [==============================] - 724s 6s/step - loss: 0.0368 - accuracy: 0.9945 - val_loss: 0.0518 - val_accuracy: 0.9867\n",
            "Epoch 8/8\n",
            "113/113 [==============================] - 737s 7s/step - loss: 0.0289 - accuracy: 0.9978 - val_loss: 0.0520 - val_accuracy: 0.9822\n",
            "Fitting fold 4 is DONE!! Time: 5793.92s\n",
            "29/29 [==============================] - 50s 2s/step - loss: 0.0520 - accuracy: 0.9822\n",
            "29/29 [==============================] - 50s 2s/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
            "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Map: 100%|██████████| 903/903 [00:00<00:00, 3175.90 examples/s]\n",
            "Map: 100%|██████████| 225/225 [00:00<00:00, 4429.46 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "113/113 [==============================] - 733s 6s/step - loss: 0.6923 - accuracy: 0.5116 - val_loss: 0.6854 - val_accuracy: 0.5333\n",
            "Epoch 2/8\n",
            "113/113 [==============================] - 759s 7s/step - loss: 0.5243 - accuracy: 0.7110 - val_loss: 0.3540 - val_accuracy: 0.8444\n",
            "Epoch 3/8\n",
            "113/113 [==============================] - 730s 6s/step - loss: 0.2017 - accuracy: 0.9435 - val_loss: 0.1557 - val_accuracy: 0.9556\n",
            "Epoch 4/8\n",
            "113/113 [==============================] - 712s 6s/step - loss: 0.1057 - accuracy: 0.9790 - val_loss: 0.1531 - val_accuracy: 0.9422\n",
            "Epoch 5/8\n",
            "113/113 [==============================] - 716s 6s/step - loss: 0.0730 - accuracy: 0.9845 - val_loss: 0.0592 - val_accuracy: 0.9778\n",
            "Epoch 6/8\n",
            "113/113 [==============================] - 722s 6s/step - loss: 0.0527 - accuracy: 0.9889 - val_loss: 0.0565 - val_accuracy: 0.9867\n",
            "Epoch 7/8\n",
            "113/113 [==============================] - 712s 6s/step - loss: 0.0468 - accuracy: 0.9900 - val_loss: 0.0845 - val_accuracy: 0.9689\n",
            "Epoch 8/8\n",
            "113/113 [==============================] - 681s 6s/step - loss: 0.0360 - accuracy: 0.9911 - val_loss: 0.0555 - val_accuracy: 0.9822\n",
            "Fitting fold 5 is DONE!! Time: 5765.15s\n",
            "29/29 [==============================] - 48s 2s/step - loss: 0.0555 - accuracy: 0.9822\n",
            "29/29 [==============================] - 48s 2s/step\n",
            "Mean Accuracy: 0.97607866273353\n",
            "Mean Precision: 0.9764127761374786\n",
            "Mean Recall: 0.97607866273353\n",
            "Mean F1 Score: 0.9760738328827715\n"
          ]
        }
      ],
      "source": [
        "# this here uses manual cross validation!\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import datasets\n",
        "import time\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "# changed epochs from 5 to 8 at June 6th\n",
        "EPOCHS = 8\n",
        "N_SPLITS = 5\n",
        "\n",
        "# Shuffle and split data into N_SPLITS folds\n",
        "indices = np.arange(len(X))\n",
        "np.random.shuffle(indices)\n",
        "folds = np.array_split(indices, N_SPLITS)\n",
        "\n",
        "# Initialize lists to store evaluation metrics\n",
        "all_val_preds = []\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1s = []\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example['hyp'], text_pair=example['tgt'], truncation=True, padding='max_length', max_length=256)\n",
        "\n",
        "def order(inp):\n",
        "    data = list(inp.values())\n",
        "    return {\n",
        "        'input_ids': tf.convert_to_tensor(data[1], dtype=tf.int32),\n",
        "        'attention_mask': tf.convert_to_tensor(data[2], dtype=tf.int32),\n",
        "    }, data[0]\n",
        "# changed from optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-5), to without legacy\n",
        "# bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=1)\n",
        "# bert_model.compile(\n",
        "#     # optimizer=Adam,\n",
        "#     loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "#     metrics=['accuracy']\n",
        "# )\n",
        "\n",
        "for i in range(N_SPLITS):\n",
        "    # Reset model for each fold\n",
        "    roberta_model = TFRobertaForSequenceClassification.from_pretrained(checkpoint, num_labels=1)\n",
        "    \n",
        "    # Compile the model with appropriate loss function and optimizer\n",
        "    roberta_model.compile(\n",
        "        optimizer=adam,\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    # the code below resets model, and I might try it out? maybe idk\n",
        "    # Prepare training and validation indices\n",
        "    val_indices = folds[i]\n",
        "    train_indices = np.hstack([folds[j] for j in range(N_SPLITS) if j != i])\n",
        "\n",
        "    # Split the data\n",
        "    X_train2, X_val = X[train_indices], X[val_indices]\n",
        "    y_train2, y_val = y[train_indices], y[val_indices]\n",
        "    # Create Pandas DataFrames\n",
        "    X_train2_df = pd.DataFrame({'hyp': X_train2[:,0], 'tgt':X_train2[:,1], 'label': y_train2})\n",
        "    X_val_df = pd.DataFrame({'hyp': X_val[:,0], 'tgt': X_val[:,1], 'label': y_val})\n",
        "\n",
        "    # Convert to HuggingFace Datasets\n",
        "    df_train = datasets.Dataset.from_pandas(X_train2_df)\n",
        "    df_val = datasets.Dataset.from_pandas(X_val_df)\n",
        "    dataset = datasets.DatasetDict({'train': df_train, 'val': df_val})\n",
        "\n",
        "    # Tokenize the datasets\n",
        "    tokenized_datasets_mapped = dataset.map(tokenize_function, batched=True, batch_size=BATCH_SIZE, remove_columns=['hyp','tgt'])\n",
        "    # Convert to TensorFlow Datasets\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(tokenized_datasets_mapped['train'][:])\n",
        "    train_dataset = train_dataset.batch(BATCH_SIZE).shuffle(1000)\n",
        "    train_dataset = train_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices(tokenized_datasets_mapped['val'][:])\n",
        "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
        "    val_dataset = val_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # Train the model\n",
        "    start = time.time()\n",
        "    roberta_model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS)\n",
        "    end = time.time()\n",
        "    elapsed_time = round(end - start, 2)\n",
        "    print(f'Fitting fold {i+1} is DONE!! Time: {elapsed_time}s')\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluated = roberta_model.evaluate(val_dataset)\n",
        "    val_preds = roberta_model.predict(val_dataset).logits\n",
        "#     val_preds = tf.nn.softmax(val_preds, axis=1).numpy()[:, 1]  # Probability of class 1, 1 try\n",
        "    # val_preds = tf.nn.sigmoid(val_preds).numpy().squeeze()[:, 1] # 2nd attempt, also i might need try out\n",
        "    # using a [:, 0] instead?\n",
        "    # Apply sigmoid activation\n",
        "    val_preds = tf.nn.sigmoid(val_preds).numpy()\n",
        "\n",
        "    # Check if val_preds has more than one dimension\n",
        "    if val_preds.ndim > 1:\n",
        "        val_preds = val_preds.squeeze()\n",
        "        # Ensure that there are at least two dimensions to index\n",
        "        if val_preds.ndim > 1:\n",
        "            val_preds = val_preds[:, 1]\n",
        "        else:\n",
        "            # Handle case where squeezing resulted in a single dimension\n",
        "            val_preds = val_preds.flatten()\n",
        "    else:\n",
        "        val_preds = val_preds.flatten()\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_val, val_preds.round(), average='weighted')\n",
        "    acc = accuracy_score(y_val, val_preds.round())\n",
        "\n",
        "    all_val_preds.append(val_preds)\n",
        "    accuracies.append(acc)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1s.append(f1)\n",
        "\n",
        "# Calculate and print average metrics\n",
        "print(f'Mean Accuracy: {np.mean(accuracies)}')\n",
        "print(f'Mean Precision: {np.mean(precisions)}')\n",
        "print(f'Mean Recall: {np.mean(recalls)}')\n",
        "print(f'Mean F1 Score: {np.mean(f1s)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "5da4f0f2",
      "metadata": {
        "id": "5da4f0f2"
      },
      "outputs": [],
      "source": [
        "# Convert to TensorFlow Datasets\n",
        "def order_final(example):\n",
        "    return {\n",
        "        'input_ids': tf.convert_to_tensor(example['input_ids'], dtype=tf.int32),\n",
        "        'attention_mask': tf.convert_to_tensor(example['attention_mask'], dtype=tf.int32),\n",
        "    }, tf.convert_to_tensor(example['label'], dtype=tf.int32)\n",
        "\n",
        "\n",
        "# final two datasets\n",
        "# ag_X, ag_y = final_ag_pg[['hyp','tgt']], final_ag_pg['label']\n",
        "ag_X, ag_y = final_ag_dm[['hyp','tgt']], final_ag_dm['label']\n",
        "# ag_X, ag_y = final_ag_mt[['hyp','tgt']], final_ag_mt['label']\n",
        "\n",
        "# ag_X, ag_y = final_aw_pg['hyp'], final_aw_pg['label']\n",
        "# ag_X, ag_y = final_aw_dm['hyp'], final_aw_dm['label']\n",
        "# ag_X, ag_y = final_aw_mt['hyp'], final_aw_mt['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "1e47d5cf",
      "metadata": {
        "id": "1e47d5cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label\n",
            "1    288\n",
            "0    275\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(ag_y.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "4476a729",
      "metadata": {
        "id": "4476a729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9761\n",
            "Precision: 0.9764\n",
            "Recall: 0.9761\n",
            "F1 Score: 0.9761\n"
          ]
        }
      ],
      "source": [
        "# i guess right now the big question is, is there a point in using cross validation?\n",
        "# unless to get best hyper paramter, which i believe i can use some other library for that,\n",
        "# what is the point? chatgpt said training final model invovles just using only\n",
        "# train test split so...\n",
        "\n",
        "print(f'Accuracy: {np.mean(accuracies):.4f}')\n",
        "print(f'Precision: {np.mean(precisions):.4f}')\n",
        "print(f'Recall: {np.mean(recalls):.4f}')\n",
        "print(f'F1 Score: {np.mean(f1s):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "033d55f1",
      "metadata": {
        "id": "033d55f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 563/563 [00:00<00:00, 4076.91 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "71/71 [==============================] - 60s 839ms/step\n",
            "Final aggregate accuracy RoBERTA: 0.4991119005328597\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score\n",
        "import datasets\n",
        "\n",
        "# Ensure eager execution\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "# Create Pandas DataFrame without the index\n",
        "final_df = pd.DataFrame({'hyp': ag_X['hyp'], 'tgt': ag_X['tgt'], 'label': ag_y})\n",
        "\n",
        "# Convert to HuggingFace Datasets\n",
        "final_df2 = datasets.Dataset.from_pandas(final_df)\n",
        "dataset = datasets.DatasetDict({'final': final_df2})\n",
        "\n",
        "# Initialize tokenizer (make sure you have the right model name)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the datasets\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example['hyp'], truncation=True, padding='max_length', max_length=100)\n",
        "\n",
        "tokenized_datasets_mapped = dataset.map(tokenize_function, batched=True, batch_size=BATCH_SIZE, remove_columns=['hyp','tgt'])\n",
        "\n",
        "# Use from_generator to create dataset with correct output signature\n",
        "# final_dataset = tf.data.Dataset.from_generator(\n",
        "#     lambda: iter(tokenized_datasets_mapped['final']),\n",
        "#     output_signature=(\n",
        "#         {\n",
        "#             'input_ids': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "#             'attention_mask': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "#             'token_type_ids': tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
        "#         },\n",
        "#         tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "#     )\n",
        "# )\n",
        "final_dataset = tf.data.Dataset.from_tensor_slices(tokenized_datasets_mapped['final'][:])\n",
        "final_dataset = final_dataset.batch(BATCH_SIZE)\n",
        "final_dataset = final_dataset.map(order_final, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Evaluate the model\n",
        "final_preds = roberta_model.predict(final_dataset).logits\n",
        "\n",
        "# Apply sigmoid to get probabilities and flatten the tensor\n",
        "final_preds = tf.nn.sigmoid(final_preds).numpy().squeeze()\n",
        "\n",
        "# If final_preds is still multidimensional, reduce it to a 1D array\n",
        "if final_preds.ndim > 1:\n",
        "    final_preds = final_preds[:, 0]\n",
        "final_preds_binary = final_preds.round()\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(ag_y, final_preds_binary)\n",
        "print(f'Final aggregate accuracy RoBERTA: {accuracy}')\n",
        "\n",
        "# Roberta, train = pg_sr, val = final_ag_pg: 0.731\n",
        "# train = pg_sr, val = final_ag_dm: 0.485\n",
        "# train = pg_sr, val = final_ag_mt: 0.591\n",
        "\n",
        "# train = dm_sr, val = final_ag_pg: 0.741\n",
        "# train = dm_sr, val = final_ag_dm: 0.504\n",
        "# train = dm_sr, val = final_ag_mt: 0.585\n",
        "\n",
        "# train = mt_sr, val = final_ag_pg: 0.741\n",
        "# train = mt_sr, val = final_ag_dm: 0.\n",
        "# train = mt_sr, val = final_ag_mt: 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "775c3215",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0.], dtype=float32)"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_preds.round()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "84acc111",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "final_preds.round()\n",
        "actual_sentences = []\n",
        "\n",
        "for i, hallucinating in enumerate(final_preds_binary):\n",
        "    if hallucinating == 1:\n",
        "        actual_sentences.append({\n",
        "            'hyp': ag_X.iloc[i]['hyp'],\n",
        "            'tgt': ag_X.iloc[i]['tgt'],\n",
        "            'label': int(ag_y.iloc[i])\n",
        "        })\n",
        "print(actual_sentences) \n",
        "with open('actual_sentence_final_ag_pg.json','w') as f:\n",
        "    json.dump(actual_sentences, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24deea98",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "7aa7eed3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "all zeros BERT: 0.48845470692717585\n"
          ]
        }
      ],
      "source": [
        "all_zeros = [0 for x in final_preds]\n",
        "print(f'all zeros BERT: {accuracy_score(all_zeros, ag_y)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "cf2983cc",
      "metadata": {
        "id": "cf2983cc"
      },
      "outputs": [],
      "source": [
        "# import json\n",
        "# copy_of_predictions = list(predictions)\n",
        "# index = 0\n",
        "# for i in range(len(copy_of_predictions)):\n",
        "#     copy_of_predictions[i] = list(dict(copy_of_predictions[i]).values())\n",
        "# #     for key,value in copy_of_predictions[i].items():\n",
        "# #         copy_of_predictions[i][key] = list(value)\n",
        "\n",
        "# print(copy_of_predictions)\n",
        "# with open('mt_sr_160.json','w') as f:\n",
        "# #     f.write(copy_of_predictions)\n",
        "#     json.dump(copy_of_predictions, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "2d24cd4c",
      "metadata": {
        "id": "2d24cd4c"
      },
      "outputs": [],
      "source": [
        "# kf.get_n_splits(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "658504c4",
      "metadata": {
        "id": "658504c4"
      },
      "outputs": [],
      "source": [
        "# OLD\n",
        "\n",
        "# # Create Pandas DataFrames\n",
        "# final_df = pd.DataFrame({'hyp': ag_X['hyp'], 'tgt': ag_X['tgt'], 'label': ag_y})\n",
        "\n",
        "# # Convert to HuggingFace Datasets\n",
        "# final_df2 = datasets.Dataset.from_pandas(final_df)\n",
        "# dataset = datasets.DatasetDict({'final': final_df2})\n",
        "\n",
        "# # Tokenize the datasets\n",
        "# tokenized_datasets_mapped = dataset.map(tokenize_function, batched=True, batch_size=BATCH_SIZE, remove_columns=['hyp','tgt'])\n",
        "\n",
        "# # Convert to TensorFlow Datasets\n",
        "# final_dataset = tf.data.Dataset.from_tensor_slices(tokenized_datasets_mapped['final'][:])\n",
        "# final_dataset = final_dataset.batch(BATCH_SIZE)\n",
        "# final_dataset = final_dataset.map(order_final, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# # my attempt failed for final dataset\n",
        "# # evaluated = model.evaluate(final_dataset)\n",
        "# final_preds = roberta_model.predict(final_dataset).logits\n",
        "# final_preds = tf.nn.sigmoid(final_preds).numpy()\n",
        "# # Check if val_preds has more than one dimension\n",
        "# if final_preds.ndim > 1:\n",
        "#     final_preds = final_preds.squeeze()\n",
        "#     # Ensure that there are at least two dimensions to index\n",
        "#     if final_preds.ndim > 1:\n",
        "#         final_preds = final_preds[:, 1]\n",
        "#     else:\n",
        "#         # Handle case where squeezing resulted in a single dimension\n",
        "#         final_preds = final_preds.flatten()\n",
        "# else:\n",
        "#     final_preds = final_preds.flatten()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch_cuda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
